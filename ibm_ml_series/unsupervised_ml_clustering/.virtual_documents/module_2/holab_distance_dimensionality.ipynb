








def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn


import seaborn as sns
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt











x = np.linspace(0, 1, 100)
y = np.zeros(100)
print(f'x: {len(x)}')
len(f'y: {y}') # 1D array
plt.plot(x, y, c='black') # plot straight line
plt.text(x=0.4, y=0.1, s='r', size=48) # plot the letter 'r'
plt.xlim(left=-1,right=1)   # setting x-limit
plt.ylim(bottom=-1,top=1)   # setting y-limit


# 'make_circle' function testing

point = 0

fig = plt.gcf()      # get-current-figure (right now initiate new figure)
ax = fig.add_subplot(111, aspect='equal')

# 'ax' can be used instead 'fig.gca()' coz already created
fig.gca().add_artist(plt.Circle((0,0), 1, alpha=0.5)) # add unit-circle to plot
# ax.add_artist(plt.Circle((0,0), 1, alpha=0.5)) # plot unit-circle
ax.scatter(0, 0, s=10, color='black')   # plot one point on the plot

ax.plot(x, y, color='black')   # plot straight line
plt.text(x=0.4, y=0.1, s='r', size=48) # plot the letter 'r'
ax.set_xlim(left=-1,right=1)   # setting x-limit
ax.set_ylim(bottom=-1,top=1)   # setting y-limit
plt.xlabel("Covariate A")   # xlabel
plt.ylabel("Covariate B")   # ylabel
plt.title("Unit Circle")   # title

# Conditional check for 'point'; 'activated' or 'deactivated'
if point:
    ax.text(x=0.55, y=0.9, s="Far away", color="purple")  # plot text
    ax.scatter(x=0.85, y=0.85, s=10, color="purple")      # plot dot

plt.show()


def make_circle(point=0):
    fig = plt.gcf()      # get-current-figure (right now initiate new figure)
    ax = fig.add_subplot(111, aspect='equal')
    
    # 'ax' can be used instead 'fig.gca()' coz already created
    fig.gca().add_artist(plt.Circle((0,0), 1, alpha=0.5)) # add unit-circle to plot
    # ax.add_artist(plt.Circle((0,0), 1, alpha=0.5)) # plot unit-circle
    ax.scatter(0, 0, s=10, color='black')   # plot one point on the plot
    ax.plot(np.linspace(0, 1, 100), np.zeros(100), color='black')   # plot straight line
    plt.text(x=0.4, y=0.1, s='r', size=48) # plot the letter 'r'
    ax.set_xlim(left=-1,right=1)   # setting x-limit
    ax.set_ylim(bottom=-1,top=1)   # setting y-limit
    plt.xlabel("Covariate A")   # xlabel
    plt.ylabel("Covariate B")   # ylabel
    plt.title("Unit Circle")   # title
    
    # Conditional check for 'point'; 'activated' or 'deactivated'
    if point:
        ax.text(x=0.55, y=0.9, s="Far away", color="purple")  # plot text
        ax.scatter(x=0.85, y=0.85, s=10, color="purple")      # plot dot
    
    plt.show()
    
make_circle()





make_circle(1)





from mpl_toolkits.mplot3d import Axes3D
from itertools import product, combinations


# Create figure
fig = plt.figure(figsize=(8,8))
ax = fig.add_subplot(111, projection='3d')


list(product([1,2],[3,4],[5,6]))    # all 8 corners of the cube


np.array(list(product([1,2],[3,4],[5,6])))   # 2-dimensional array


list(combinations([1, 2, 3], 2))
# Output: [(1, 2), (1, 3), (2, 3)]


# Unique combinations without replacement
# Unique pairs of all 8 corners of the cube (28 total pairs)
for _ in combinations(np.array(list(product([1,2],[3,4],[5,6]))), 2):
    print(_)


r = [-1, 1] # Min and Max coordinates of the cube
for s, e in combinations(np.array(list(product(r,r,r))), 2):
    if np.sum(np.abs(s-e)) == r[1] - r[0]:
        print(f"without '*': {list(zip(s,e))} | with '*': {list(zip(*zip(s,e)))}")
        


fig = plt.figure(figsize=(8,8))
ax = fig.add_subplot(111, projection='3d')

# Draw cube
r = [-1, 1] # Min and Max coordinates of the cube
for s, e in combinations(np.array(list(product(r,r,r))), 2):
    if np.sum(np.abs(s-e)) == r[1] - r[0]:
        ax.plot3D(*zip(s,e))   # plot a line in 3D from point 's' to point 'e'


np.mgrid[0:10:6j]   # j means evenly space


fig = plt.figure(figsize=(8,8))
ax = fig.add_subplot(111, projection='3d')

# Draw sphere 
u, v = np.mgrid[0:2 * np.pi:20j, 0:np.pi:10j] # 2D meshgrid
x = np.cos(u) * np.sin(v)
y = np.sin(u) * np.sin(v)
z = np.cos(v)
# ax.plot(x, y, z, color='black')
ax.plot_wireframe(x, y, z, color='black')
plt.show()


fig = plt.figure(figsize=(8,8))
ax = fig.add_subplot(111, projection='3d')

# Draw cube
r = [-1, 1] # Min and Max coordinates of the cube
for s, e in combinations(np.array(list(product(r,r,r))), 2):
    if np.sum(np.abs(s-e)) == r[1] - r[0]:
        # print(f"without '*': {list(zip(s,e))} | with '*': {list(zip(*zip(s,e)))}")
        # '*' means unpacking or transpose data
        ax.plot3D(*zip(s,e))   # plot a line in 3D from point 's' to point 'e'

# Draw sphere on the same axis
u, v = np.mgrid[0:2*np.pi:20j, 0:np.pi:10j]
x=np.cos(u)*np.sin(v)
y=np.sin(u)*np.sin(v)
z=np.cos(v)
ax.plot_wireframe(x, y, z, color="black");

plt.show()





# Draw a sample of data in two dimensions
sample_data = np.random.sample((5,2))   # 5 rows x 2 columns
print("Sample data: \n", sample_data, '\n')


def norm(x):
    ''' Measure the distance of each point from the origin.
    
    Input: Sample points, one point per row
    Output: The distance from the origin to each point
    '''
    return np.sqrt((x**2).sum(1))


def in_the_ball(x): 
    ''' Determine if the sample is in the circle. 
    
    Input: Sample points, one point per row
    Output: A boolean array indicating whether the point is in the ball
    '''
    return norm(x) < 1 # If the distance measure above is <1, we're inside the ball


for x, y in zip(norm(sample_data),in_the_ball(sample_data)):
    print("Norm = ", x.round(2), "; is in circle? ", y) 








shape = 5, 1
np.random.sample(shape)


def what_percent_of_the_ncube_is_in_the_nball(d_dim,
                                              sample_size=5):
    # print(f"sample_size: {sample_size} | d_dim: {d_dim}")
    shape = sample_size, d_dim
    # print("shape:", shape)
    for i in range(2):
        # print("shape:", shape)
        # print("i:", i)
        sample = np.random.sample(shape)
        # print("sample:", sample)

        # norm() function
        ball_ = np.sqrt((sample**2).sum(1))
        # print("ball_", ball_)
        
        # in_the_ball() function
        bool_ = ball_ < 1        
        # print("bool_:", bool_)
        
        mean = bool_.mean()
        # print("mean:", mean)
        # print('*' * 30)

        return mean


dims = range(0, 10)
# 'map()' function looks like 'for loop'
data = np.array(list(map(what_percent_of_the_ncube_is_in_the_nball, dims)))
data


for dim, percent in zip(dims, data):
    print("Dimension = ", dim, "; percent in ball = ", percent)


def what_percent_of_the_ncube_is_in_the_nball(d_dim,
                                              sample_size=10**4):
    # To create random samples
    shape = sample_size, d_dim   

    # Record 100 mean values of the random sample points
    data = np.array([in_the_ball(np.random.sample(shape)).mean()
                     for iteration in range(100)])
    # Return mean values as an array
    return data.mean()

dims = range(2,15)  # 2 to 14 dimensions

# 'map()'' function looks like 'for loop'
data = np.array(list(map(what_percent_of_the_ncube_is_in_the_nball,dims)))


for dim, percent in zip(dims,data):
    print("Dimension = ", dim, "| percent in ball = ", percent)


# Plotting dimension vs mean values
plt.plot(dims, data, color='black')
plt.scatter(dims, data, color='blue')
plt.xlabel('# dimensions')
plt.ylabel("% of area in shpere")
plt.title("What percentage of the cube is in the sphere?")
plt.show()








def get_min_distance(dimension, sample_size=10**3):
    ''' Sample some random points and find the closet 
    of those random points to the center of the data '''
    points = np.random.sample((sample_size,dimension))-.5   # centering our data
    return np.min(norm(points))

def estimate_closest(dimension):
    ''' For a given dimension, take a random sample in that dimension and then find 
        that sample's closest point to the center of the data. 
        Repeat 100 times for the given dimension and return the min/max/mean 
        of the distance for the nearest point. '''
    data = np.array([get_min_distance(dimension) for _ in range(100)])
    return data.mean(), data.min(), data.max()

# Calculate for dimensions 2-100
dims = range(2,100)
min_distance_data = np.array(list(map(estimate_closest,dims)))

# Test it for dimension 6
print("For dimension 6: ", estimate_closest(6))


# Plot the min/max/mean of the closest point for each dimension using sampling 

plt.plot(dims,min_distance_data[:,0], color='blue')
plt.fill_between(dims, min_distance_data[:,1], min_distance_data[:,2],alpha=.5)
plt.xlabel("# dimensions")
plt.ylabel("Estimated min distance from origin")
plt.title("How far away from the origin is the closest point?"); 











import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_classification
from sklearn.tree import DecisionTreeClassifier


# Create a dataset with two features 

X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,
                           random_state=1, n_clusters_per_class=2)
rng = np.random.RandomState(2)
X += 2 * rng.uniform(size=X.shape)

X = StandardScaler().fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=42)


DT =  DecisionTreeClassifier()
DT.fit(X_train, y_train)
score = DT.score(X_test, y_test)


print("Score from two-feature classifier: ", score)


# Now do the same thing but with 200 features 

X, y = make_classification(n_features=200, n_redundant=0, n_informative=200,
                           random_state=1, n_clusters_per_class=2)
rng = np.random.RandomState(2)
X += 2 * rng.uniform(size=X.shape)

X = StandardScaler().fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=42)

DT =  DecisionTreeClassifier()
DT.fit(X_train, y_train)
score = DT.score(X_test, y_test)


print("Score from 200-feature classifier: ", score)





scores = []

increment, max_features = 50, 4000

for num in np.linspace(increment, max_features, increment, dtype='int'):

    X, y = make_classification(n_features=num, n_redundant=0, 
                               random_state=1, n_clusters_per_class=1, n_classes = 3)
    rng = np.random.RandomState(2)
    X += 2 * rng.uniform(size=X.shape)

    X = StandardScaler().fit_transform(X)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=42)

    
    DT =  DecisionTreeClassifier()
    DT.fit(X_train, y_train)
    scores.append( DT.score(X_test, y_test) )



plt.plot(np.linspace(increment, max_features, increment, dtype='int'),scores)
plt.title("Accuracy of Classification with Increasing Features")
plt.xlabel("Number of features")
plt.ylabel("Classification accuracy");



