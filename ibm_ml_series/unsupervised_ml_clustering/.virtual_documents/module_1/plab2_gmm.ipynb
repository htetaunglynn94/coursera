
































# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.
# !mamba install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1 wget
# Note: If your environment doesn't support "!mamba install", use "!pip install pandas==1.3.4 ..."








def warn(*args, **kwargs):
    pass


import warnings
warnings.filterwarnings('ignore')


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.mixture import GaussianMixture as GM
from sklearn.preprocessing import StandardScaler as SS
from scipy.stats import norm, multivariate_normal
from itertools import chain
from matplotlib.patches import Ellipse


# You can also use this section to suppress warnings generated by your code:
import warnings
warnings.filterwarnings('ignore')


import numpy as np
import pandas as pd
import scipy.stats as ss
import matplotlib.pyplot as plt
%matplotlib inline
from sklearn.mixture import GaussianMixture
import seaborn as sns

from sklearn.preprocessing import StandardScaler
from scipy.stats import multivariate_normal
from itertools import chain
from matplotlib.patches import Ellipse


sns.set_context('notebook')
sns.set_style('white')

def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')





# This function will allow us to easily plot data taking in x values, y values, and a title
def plot_univariate_mixture(means, stds, weights, N = 10000, seed=10):
    
    """
    returns the simulated 1d dataset X, a figure, and the figure's ax
    
    """
    np.random.seed(seed)
    if not len(means)==len(stds)==len(weights):
        raise Exception("Length of mean, std, and weights don't match.") 
    K = len(means)
    
    mixture_idx = np.random.choice(K, size=N, replace=True, p=weights)
    # generate N possible values of the mixture
    X = np.fromiter((ss.norm.rvs(loc=means[i], scale=stds[i]) for i in mixture_idx), dtype=np.float64)
      
    # generate values on the x axis of the plot
    xs = np.linspace(X.min(), X.max(), 300)
    ps = np.zeros_like(xs)
    
    for mu, s, w in zip(means, stds, weights):
        ps += ss.norm.pdf(xs, loc=mu, scale=s) * w
    
    fig, ax = plt.subplots()
    ax.plot(xs, ps, label='pdf of the Gaussian mixture')
    ax.set_xlabel("X", fontsize=15)
    ax.set_ylabel("P", fontsize=15)
    ax.set_title("Univariate Gaussian mixture", fontsize=15)
    #plt.show()
    
    return X.reshape(-1,1), fig, ax
    
    
def plot_bivariate_mixture(means, covs, weights, N = 10000, seed=10):
    
    """
    returns the simulated 2d dataset X and a scatter plot is shown
    
    """
    np.random.seed(seed)
    if not len(mean)==len(covs)==len(weights):
        raise Exception("Length of mean, std, and weights don't match.") 
    K = len(means)
    M = len(means[0])
    
    mixture_idx = np.random.choice(K, size=N, replace=True, p=weights)
    
    # generate N possible values of the mixture
    X = np.fromiter(chain.from_iterable(multivariate_normal.rvs(mean=means[i], cov=covs[i]) for i in mixture_idx), 
                dtype=float)
    X.shape = N, M
    
    xs1 = X[:,0] 
    xs2 = X[:,1]
    
    plt.scatter(xs1, xs2, label="data")
    
    L = len(means)
    for l, pair in enumerate(means):
        plt.scatter(pair[0], pair[1], color='red')
        if l == L-1:
            break
    plt.scatter(pair[0], pair[1], color='red', label="mean")
    
    plt.xlabel("$x_1$")
    plt.ylabel("$x_2$")
    plt.title("Scatter plot of the bivariate Gaussian mixture")
    plt.legend()
    plt.show()
    
    return X


def draw_ellipse(position, covariance, ax=None, **kwargs):
    
    """
    Draw an ellipse with a given position and covariance
    
    """
    ax = ax or plt.gca()
    
    # Convert covariance to principal axes
    if covariance.shape == (2, 2):
        U, s, Vt = np.linalg.svd(covariance)
        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))
        width, height = 2 * np.sqrt(s)
    else:
        angle = 0
        width, height = 2 * np.sqrt(covariance)
    
    # Draw the Ellipse
    for nsig in range(1, 4):
        ax.add_patch(Ellipse(position, nsig * width, nsig * height, angle, **kwargs))
        
        
def plot_gmm(gmm, X, label=True, ax=None):
    ax = ax or plt.gca()
    labels = gmm.fit(X).predict(X)
    if label:
        ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)
    else:
        ax.scatter(X[:, 0], X[:, 1], s=40, zorder=2)
    ax.axis('equal')
    
    w_factor = 0.2 / gmm.weights_.max()
    for pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_):
        draw_ellipse(pos, covar, alpha=w * w_factor)


















def plot_univariate_mixture(means, stds, weights, N = 10000, seed=10):
    """
    Returns the simulated 1d dataset X, a figure, and the figure's ax
    """
   
    np.random.seed(seed)
    
    if not len(means) == len(stds) == len(weights):
        raise Exception("Length of mean, std, and weights don't match.") 
    K = len(means)  # 3 values
    
    # Generate 1000 random values
    mixture_idx = np.random.choice(a = K,            # works as np.arange(a) [0,1,2]
                                   size = N,         # size of the random array values
                                   replace = True,   # default
                                   p = weights)      # probabilities
    
    # Generate N possible values of the mixture
    # rvs = random variates
    X = np.fromiter((norm.rvs(loc = means[i],      # center of the distribution
                              scale = stds[i]) for i in mixture_idx), # standard deviation 
                              # (most values between (mean Â± 2))
                    dtype = np.float64)
    
    # Generate values on the x axis of the plot
    xs = np.linspace(X.min(), X.max(), 300)
    ps = np.zeros_like(xs)    # same with np.zeros(xs.shape)
    
    for mu, s, w in zip(means, stds, weights):
        # Probability Density Function
        ps += norm.pdf(xs, loc=mu, scale=s) * w
        
    fig, ax = plt.subplots()
    ax.plot(xs, ps, label='pdf of the Gaussian mixture')
    ax.set_xlabel("X", fontsize=15)
    ax.set_ylabel("P", fontsize=15)
    ax.set_title("Univariate Gaussian mixture", fontsize=15)
    # plt.show()

    return X.reshape(-1,1), fig, ax


X1, fig1, ax1 = plot_univariate_mixture(means=[2,5,8], stds=[0.2,0.5,0.8], weights=[0.3,0.3,0.4]) 





X2, fig2, ax2 = plot_univariate_mixture(means=[2,5,8], stds=[0.6,0.9,1.2], weights=[0.3,0.3,0.4]) 





X3, fig3, ax3 = plot_univariate_mixture(means=[2,5,8], stds=[0.6,0.9,1.2], weights=[0.05,0.35,0.6]) 

















X1.shape


X1.reshape(-1).shape


# Sort X1 in ascending order for plotting purpose
X1_sorted = np.sort(X1[:,0]).reshape(-1,1)


# Fit the GMM
gm = GM(n_components=3, random_state=10).fit(X1_sorted)


# Store the predicted probabilities in prob_X1
prob_X1 = gm.predict_proba(X1_sorted)
prob_X1


# Plotting
ax1.plot(X1_sorted, prob_X1[:, 0], label = 'Predicted prob: of x belonging to cluster 1')
ax1.plot(X1_sorted, prob_X1[:, 1], label = 'Predicted prob: of x belonging to cluster 2')
ax1.plot(X1_sorted, prob_X1[:, 2], label = 'Predicted prob: of x belonging to cluster 3')
ax1.scatter(2, 0.6, color='black')
ax1.scatter(2, 1.0, color='black')
ax1.plot([2,2], [0.6,1.0], '--', color='black')
ax1.legend()
fig1

















# Set parameter values for Gaussian mixture data
mean = [(1,5), (2,1), (6,2)]
cov1 = np.array([[0.5, 1.0], [1.0, 0.8]])  # standard deviation 1
cov2 = np.array([[0.8, 0.4], [0.4, 1.2]])  # standard deviation 2
cov3 = np.array([[1.2, 1.3], [1.3, 0.9]])  # standard deviation 3

def fix_covariance(matrix):
    sym = (matrix + matrix.T) / 2  # Make symmetric
    eigvals, eigvecs = np.linalg.eigh(sym)
    eigvals[eigvals < 0] = 0       # Remove negative values
    return eigvecs @ np.diag(eigvals) @ eigvecs.T

cov1 = fix_covariance(cov1)
cov2 = fix_covariance(cov2)
cov3 = fix_covariance(cov3)

cov = [cov1, cov2, cov3]

weights = [0.3, 0.3, 0.4]





def plot_bivariate_mixture(means, covs, weights, N = 10000, seed=10):

    """
    returns the simulated 2d dataset X and a scatter plot is shown

    """

    np.random.seed(seed)

    if not len(mean) == len(covs) == len(weights):
        raise Exception("Length of mean, std and weights don't match.")

    K = len(means)
    M = len(means[0])

    # Generate K random values
    mixture_idx = np.random.choice(a = K,            # works as np.arange(a) [0,1,2]
                                   size = N,         # size of the random array values
                                   replace = True,   # default
                                   p = weights)      # probabilities

    X = np.fromiter(chain.from_iterable(multivariate_normal.rvs(mean=means[i], cov=covs[i]) for i in mixture_idx), 
                    dtype=float)

    X.shape = N, M # N = 1000, M = 2; 1D to 2D
    xs1 = X[:,0]
    xs2 = X[:,1]
    plt.scatter(xs1, xs2, label='data')  # random generated data plotting

    # Plot centroids
    for _, pair in enumerate(means):
        plt.scatter(pair[0], pair[1], color = 'red')
        if _ == K - 1:
            break
    plt.scatter(pair[0], pair[1], color='red', label='mean') # to show 'mean' in legend one time
    plt.xlabel("$x_1$")
    plt.ylabel("$x_2$")
    plt.title("Scatter plot of the bivariate Gaussian mixture")
    plt.legend()
    plt.show()
    return X


a = [1,2]
b = [3,4]
r = list(chain(a, b))
print(r)

# Flattening a nested list
nested = [[1, 2], [3, 4]]
flat = list(chain(*nested))  # '*' means unpacking
flat


def plot_bivariate_mixture(means, covs, weights, N = 10000, seed=10):
    
    """
    returns the simulated 2d dataset X and a scatter plot is shown
    
    """
    np.random.seed(seed)
    if not len(mean)==len(covs)==len(weights):
        raise Exception("Length of mean, std, and weights don't match.") 
    K = len(means)
    M = len(means[0])
    
    mixture_idx = np.random.choice(K, size=N, replace=True, p=weights)
    
    # generate N possible values of the mixture
    X = np.fromiter(chain.from_iterable(multivariate_normal.rvs(mean=means[i], cov=covs[i]) for i in mixture_idx), 
                dtype=float)
    X.shape = N, M
    
    xs1 = X[:,0] 
    xs2 = X[:,1]
    
    plt.scatter(xs1, xs2, label="data")
    
    L = len(means)
    for l, pair in enumerate(means):
        plt.scatter(pair[0], pair[1], color='red')
        if l == L-1:
            break
    plt.scatter(pair[0], pair[1], color='red', label="mean")
    
    plt.xlabel("$x_1$")
    plt.ylabel("$x_2$")
    plt.title("Scatter plot of the bivariate Gaussian mixture")
    plt.legend()
    plt.show()
    
    return X
    


X4 = plot_bivariate_mixture(means=mean, covs=cov, weights=weights, N=1000)





print("The dataset we generated has a shape of", X4.shape)





gm = GM(n_components=3, random_state=0).fit(X4)
print(f"Means of the 3 Gaussians fitted by GMM are \n{gm.means_}.")


print(f"Covariances of the 3 Gaussians fitted by GMM are \n {gm.covariances_}.")














def draw_ellipse(position, covariance, ax=None, **kwargs):
    
    """
    Draw an ellipse with a given position and covariance
    
    """
    ax = ax or plt.gca()
    
    # Convert covariance to principal axes
    if covariance.shape == (2, 2):
        # Break matrix into rotation, scale and rotate
        
        # svd = Singular Value Decomposition
        # U = left singular vector, s = singular values, Vt = right singular vector
        U, s, Vt = np.linalg.svd(covariance)
        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))
        width, height = 2 * np.sqrt(s)
    else:
        angle = 0
        width, height = 2 * np.sqrt(covariance)
    
    # Draw the Ellipse
    for nsig in range(1, 4):
        ax.add_patch(Ellipse(                         # add custom ellipse shape to the plot
                             xy=position,             # center of ellipse (mean value of Gaussian component)
                             width = nsig * width,    # width of ellipse scaled by current sigma level
                             height = nsig * height,  # height of ellipse scaled by current sigma level
                             angle = angle,           # rotation angle of ellipse
                             **kwargs))               # allow to pass extra parameters like
                                                      # alpha=0.5, color='blue', edgecolor='black', etc.
        
def plot_gmm(gmm, X, label=True, ax=None):
    ax = ax or plt.gca()
    labels = gmm.fit(X).predict(X)
    if label:
        ax.scatter(X[:, 0], X[:, 1],  # data values
                   c=labels,          # lable identification
                   s=40,              # size
                   cmap='viridis',    # color map
                   zorder=2)          # stacking order
    else:
        ax.scatter(X[:, 0], X[:, 1], s=40, zorder=2)
    ax.axis('equal') # equal scaling on both axes
    
    w_factor = 0.2 / gmm.weights_.max()
    for pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_):
        draw_ellipse(pos, covar, alpha=w * w_factor)


plot_gmm(GM(n_components=3, random_state=0, 
            covariance_type='full'), # default 
          X4, label=True) # simulated Gaussian mixture data











# try Covariance_type = 'tied'
plot_gmm(GM(n_components=3, covariance_type='tied', random_state=0), # the model, 
         X4)


# try Covariance_type = 'diag'
plot_gmm(GM(n_components=3, covariance_type='diag', random_state=0), # the model, 
         X4)














! wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%201/images/gauss-cat.jpeg





img = plt.imread('gauss-cat.jpeg')

X = img.reshape(-1, 3)
n = 2
gm = GM(n_components=n, covariance_type='tied').fit(X)
labels = gm.predict(X)





seg = np.zeros(X.shape) # num of pixels x 3

for label in range(n):
    seg[labels == label] = gmm.means_[label]
seg = seg.reshape(img.shape).astype(np.uint8)

plt.figure(figsize=(6,6))
plt.imshow(seg)





n = 8
gmm = GaussianMixture(n_components=n, covariance_type='tied')
gmm.fit(X)
labels = gmm.predict(X) # num of pixels x 1
seg = np.zeros(X.shape) # num of pixels x 3

for label in range(n):
    seg[labels == label] = gmm.means_[label]
seg = seg.reshape(img.shape).astype(np.uint8)
#cv2.imwrite(f'gauss-cat-{n}.jpeg', seg)

plt.figure(figsize=(6,6))
plt.imshow(seg)
plt.show()














data = pd.read_csv("https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%201/customers.csv")
data.head()

# you can also download the csv file to your local workspace using:
# ! wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%201/customers.csv


data.shape





from sklearn.preprocessing import StandardScaler
SS = # TODO 
X = # TODO 








from sklearn.decomposition import PCA
pca2 = # TODO 
reduced_2_PCA = # TODO 








model = # TODO 









PCA_2_pred = # TODO 








x = # TODO 
y = # TODO 
plt.scatter(x, y, c=PCA_2_pred)
plt.title("2d visualization of the clusters")
plt.xlabel("PCA 1")
plt.ylabel("PCA 2")








# use PCA with n=3 and implement GMM to make predictions

pca3 = # TODO 
reduced_3_PCA = # TODO 
mod = # TODO 
PCA_3_pred = # TODO 

# plotting

reduced_3_PCA = pd.DataFrame(reduced_3_PCA, columns=(['PCA 1', 'PCA 2', 'PCA 3']))
fig = plt.figure(figsize=(10,8))
ax = fig.add_subplot(111, projection="3d")
ax.scatter(reduced_3_PCA['PCA 1'],reduced_3_PCA['PCA 2'],reduced_3_PCA['PCA 3'], c=PCA_3_pred)
ax.set_title("3D projection of the clusters")





















