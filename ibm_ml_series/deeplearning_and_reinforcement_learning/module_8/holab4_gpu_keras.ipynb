{"cells":[{"cell_type":"markdown","metadata":{"id":"b19eea5d-1eca-41ac-a4c8-81c0c9eaa6b1"},"source":["<center>\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"300\" alt=\"cognitiveclass.ai logo\">\n","</center>\n"]},{"cell_type":"markdown","metadata":{"id":"a980c479-dfb6-45dc-92c4-90ad21830cf6"},"source":["# **GPU with Keras**\n"]},{"cell_type":"markdown","metadata":{"id":"419333e7-5273-4d29-9758-2f6e842e2f50"},"source":["Estimated time needed: **25** minutes\n"]},{"cell_type":"markdown","metadata":{"id":"63a99521-c452-461a-a7bc-3127b66979ab"},"source":["You may have heard of GPUs (Graphics Processing Unit) and CPUs (Central Processing Unit), but what is the difference? GPUs have been commonly seen used by gamers for better visual rendering, but nowadays its applications extend way beyond improving videogame experience. With respect to deep learning, GPUs are extremely helpful by speeding up certain computations. The difference is evident especially for models that train on large datasets, in which the researcher can take advantage of parallel computing to run operations simultaneously and save time. In this lab, you will learn about how to utilize GPU for `tensorflow`, specifically `keras`.\n"]},{"cell_type":"markdown","metadata":{"id":"2e011941-936b-4629-82a0-314bd505afcd"},"source":["<center>\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module6/L1/img_GPU.jpeg\" width=\"600\" alt=\"computer components\">\n","<center>\n"]},{"cell_type":"markdown","metadata":{"id":"e0d1315e-2222-46d0-a88a-e487ff723b0f"},"source":["**_Note_**: Skills Network Labs currently doesn't have any GPUs available. In order to test the difference between CPU and GPU, please run this lab on a local machine or environment that has GPUs.\n"]},{"cell_type":"markdown","metadata":{"id":"7a2b2c10-4452-4156-92a9-56c754ca9514"},"source":["## __Table of Contents__\n","\n","<ol>\n","    <li><a href=\"#Objectives\">Objectives</a></li>\n","    <li>\n","        <a href=\"#Setup\">Setup</a>\n","        <ol>\n","            <li><a href=\"#Installing-Required-Libraries\">Installing Required Libraries</a></li>\n","            <li><a href=\"#Importing-Required-Libraries\">Importing Required Libraries</a></li>\n","            <li><a href=\"#Defining-Helper-Functions\">Defining Helper Functions</a></li>\n","        </ol>\n","    </li>\n","    <li>\n","        <a href=\"#Benefits-of-Using-GPU\">Benefits of Using GPU</a>\n","    </li>  \n","    <li>\n","        <a href=\"#Using-CPU\">Using CPU</a>\n","    </li>\n","    <li>\n","        <a href=\"#Using-GPU\">Using GPU</a>\n","        <ol>\n","            <li><a href=\"#Check-Availability\">Check Availability</a></li>\n","            <li><a href=\"#Choosing-Specific-GPUs\">Choosing Specific GPUs</a></li>\n","        </ol>    \n","    </li>\n","    <li>\n","        <a href=\"#Using-CPU-and-GPU-jointly\">Using CPU and GPU jointly</a>\n","    </li>     \n","</ol>\n"]},{"cell_type":"markdown","metadata":{"id":"28230b5d-8859-45af-8c58-1c0eb8f67a53"},"source":["## Objectives\n","\n","After completing this lab you will be able to:\n","\n"," - Set environment to CPU/GPU\n"," - Control usage of CPU/GPU in parts of the code\n"]},{"cell_type":"markdown","metadata":{"id":"65dac39e-540d-4162-a182-58eb3ca0f0b3"},"source":["----\n"]},{"cell_type":"markdown","metadata":{"id":"c56e4b72-4b77-4e7e-9a66-5e12479ce5d3"},"source":["## Setup\n"]},{"cell_type":"markdown","metadata":{"id":"e7b4f664-9445-4fe6-b0d2-bb3b763bf43a"},"source":["For this lab, we will be using the following libraries:\n","\n","*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for managing the data.\n","*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for mathematical operations.\n","*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for machine learning and machine-learning-pipeline related functions.\n","*   [`seaborn`](https://seaborn.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for visualizing the data.\n","*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for additional plotting tools.\n"]},{"cell_type":"markdown","metadata":{"id":"3e6c2dcc-aabb-4cc6-bcea-53e7e4ccacd8"},"source":["### Installing Required Libraries\n","\n","The following required libraries are pre-installed in the Skills Network Labs environment. However, if you run these notebook commands in a different Jupyter environment (e.g. Watson Studio or Ananconda), you will need to install these libraries by removing the `#` sign before `!mamba` and before `!pip install --upgrade tensorflow` in the code cells below.\n"]},{"cell_type":"code","metadata":{"id":"868a4448-0613-4e0d-8479-1d1d4483aed5"},"outputs":[],"source":["# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n","# !mamba install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1\n","# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\""],"execution_count":null},{"cell_type":"code","source":["%%capture\n","!pip install --upgrade tensorflow -qqq\n","# NOTE: -q   -> quiet (show fewer messages)\n","#       -qq  -> very quiet (show almost nothing)\n","#       -qqq -> super quiet (show no output unless errors occur)"],"metadata":{"id":"VTgyNal79-fz","executionInfo":{"status":"ok","timestamp":1765549941195,"user_tz":-480,"elapsed":5807,"user":{"displayName":"Htet Aung Lynn","userId":"02450750355632558897"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9a38f0bc-72ed-4f60-8115-e99431930772"},"source":["### Importing Required Libraries\n","\n","_We recommend you import all required libraries in one place, as follows:_\n"]},{"cell_type":"code","source":["import warnings\n","warnings.simplefilter('ignore')\n","\n","import os\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","import numpy as np\n","\n","import tensorflow as tf\n","# Import the keras library\n","from tensorflow import keras\n","from tensorflow.keras import Model\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n","from tensorflow.python.client import device_lib"],"metadata":{"id":"dziFPPoe-h_5","executionInfo":{"status":"ok","timestamp":1765550674767,"user_tz":-480,"elapsed":8649,"user":{"displayName":"Htet Aung Lynn","userId":"02450750355632558897"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2c547c75-bae6-465d-ae3f-c954417704e9"},"source":["## Benefits of Using GPU\n"]},{"cell_type":"markdown","metadata":{"id":"cd56d5b0-e39c-4e2a-8ff8-7530c8ffe962"},"source":["GPU excels in parallel computing in comparison to CPU. This technique is especially useful for deep learning algorithms, such as building a Convolutional Neural Network (CNN), or as a matter of fact, any neural network. An example of a parallel processing task is performing convolution on an input layer, in which the kernel is multiplied with the input layer matrix, one local region at a time.\n","\n","The runtime difference is especially noticeable when you train a CNN with multiple epochs - tasks where a lot of matrix operations are involved!\n"]},{"cell_type":"markdown","metadata":{"id":"a10dca2a-434e-434d-bfd0-9ec05274f5af"},"source":["## Using CPU\n"]},{"cell_type":"markdown","metadata":{"id":"c28b5765-04da-41ca-a4cb-bf091f372d42"},"source":["By default, `tensorflow` searches for available GPU to use. There are two ways to force your machine to ignore all GPUs and run code with CPU instead.\n"]},{"cell_type":"markdown","metadata":{"id":"2b26d0c4-ce39-436f-80cd-9f48b1f070b1"},"source":["If you want the entire code/notebook to run on CPU, you can specify the environment _**before**_ importing tensorflow/keras. If you decide to switch back, you can restart the kernel and run import as usual without the line below.\n"]},{"cell_type":"code","source":["# Specify the environment variable value\n","os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"],"metadata":{"id":"4CmkA-2--3cZ","executionInfo":{"status":"ok","timestamp":1765550049659,"user_tz":-480,"elapsed":5,"user":{"displayName":"Htet Aung Lynn","userId":"02450750355632558897"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5b3babc7-70e9-44f7-aa66-cf02c4337077"},"source":["If the environment variable `CUDA_VISIBLE_DEVICES` value takes the values 0/1 (or other positive values), the machine is using GPU to run the code. By setting it to -1, it specifies the algorithm to be run with CPU.\n"]},{"cell_type":"code","source":["# Check that CPU is used\n","print(os.environ['CUDA_VISIBLE_DEVICES'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TkWdjtFc--TI","executionInfo":{"status":"ok","timestamp":1765550058325,"user_tz":-480,"elapsed":41,"user":{"displayName":"Htet Aung Lynn","userId":"02450750355632558897"}},"outputId":"4c8d203d-54e1-4bb4-b059-7b017e7a135c"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["-1\n"]}]},{"cell_type":"markdown","metadata":{"id":"f87d2465-0a91-476d-8089-0682f30243de"},"source":["If instead you want to use CPU for portions of the code in a notebook, consider the following approach. Here, you specify what to run with `/CPU:0` using a `with` statement. Using `%%timeit` with `-n1 -r1` will time the process for one pass of the cell. As an example, we'll be training the following CNN on a **DATASET**. Feel free to change the code within the statement to test CPU performance!\n"]},{"cell_type":"code","source":["# Import data\n","(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nYXq57ar_HLO","executionInfo":{"status":"ok","timestamp":1765550723487,"user_tz":-480,"elapsed":2220,"user":{"displayName":"Htet Aung Lynn","userId":"02450750355632558897"}},"outputId":"ac9b6a1a-ed5d-4a7d-c4ee-43f509cb4749"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"]}]},{"cell_type":"code","source":["# Reshape the data\n","X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2], 1))\n","X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2], 1))\n","\n","y_train = y_train.reshape((y_train.shape[0], 1))\n","y_test = y_test.reshape((y_test.shape[0], 1))"],"metadata":{"id":"VVXh2dGB_Jmo","executionInfo":{"status":"ok","timestamp":1765550724701,"user_tz":-480,"elapsed":6,"user":{"displayName":"Htet Aung Lynn","userId":"02450750355632558897"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["%%timeit -n1 -r1\n","# NOTE: -n1 -> run the code 1 time per loop\n","#       -r1 -> repeat the loop 1 time\n","\n","# Building the CNN model and fitting on train data\n","with tf.device('/CPU:0'):\n","    model_cpu = Sequential()\n","    model_cpu.add(Conv2D(input_shape = (28, 28, 1),\n","                     filters=5,\n","                     padding='Same',\n","                     kernel_size=(3,3)\n","                     ))\n","    model_cpu.add(MaxPooling2D(pool_size=(2,2)))\n","    model_cpu.add(Flatten())\n","    model_cpu.add(Dense(256, activation='relu'))\n","    model_cpu.add(Dense(10, activation='softmax'))\n","\n","    model_cpu.compile(optimizer='adam',\n","              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","\n","    model_cpu.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9jSuhu8P_T7O","executionInfo":{"status":"ok","timestamp":1765550340675,"user_tz":-480,"elapsed":158975,"user":{"displayName":"Htet Aung Lynn","userId":"02450750355632558897"}},"outputId":"d827bf00-871b-4586-ee05-981aee2a1b62"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 17ms/step - accuracy: 0.8692 - loss: 5.1517 - val_accuracy: 0.9424 - val_loss: 0.2390\n","Epoch 2/5\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 16ms/step - accuracy: 0.9591 - loss: 0.1475 - val_accuracy: 0.9563 - val_loss: 0.1771\n","Epoch 3/5\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 16ms/step - accuracy: 0.9719 - loss: 0.0985 - val_accuracy: 0.9644 - val_loss: 0.1283\n","Epoch 4/5\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 17ms/step - accuracy: 0.9763 - loss: 0.0786 - val_accuracy: 0.9685 - val_loss: 0.1355\n","Epoch 5/5\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 18ms/step - accuracy: 0.9798 - loss: 0.0666 - val_accuracy: 0.9692 - val_loss: 0.1355\n","2min 38s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}]},{"cell_type":"markdown","metadata":{"id":"6fa8c13a-7343-4adb-889a-4967c8db3eb4"},"source":["## Using GPU\n"]},{"cell_type":"markdown","metadata":{"id":"8264a9db-9370-48bb-8daa-ca4030667a85"},"source":["As mentioned above, `tensorflow` automatically searches for GPUs to run on. Let's take a closer look at how we can have more control over that.\n"]},{"cell_type":"markdown","metadata":{"id":"ed5aa854-d956-4d3f-91ea-4892e842422f"},"source":["### Check Availability\n"]},{"cell_type":"markdown","metadata":{"id":"0ab8d633-8fd5-4821-871c-9589b273b003"},"source":["First, you can check the number of GPUs available on the machine.\n"]},{"cell_type":"code","source":["# After changing runtime type to T4 GPU\n","print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o7fsrgFdAPoE","executionInfo":{"status":"ok","timestamp":1765550682092,"user_tz":-480,"elapsed":10,"user":{"displayName":"Htet Aung Lynn","userId":"02450750355632558897"}},"outputId":"55388bdb-6aab-4ef9-d132-110d77a66843"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Num GPUs Available:  1\n"]}]},{"cell_type":"markdown","metadata":{"id":"ac2c6ca7-9f19-4396-901e-073eda98a3bf"},"source":["If you're running this notebook in Skills Network Lab, you can see that it doesn't have any GPUs available for use. However, if your local machine does have GPU(s), you can try the following code to play with what you want to run on GPU.\n"]},{"cell_type":"markdown","metadata":{"id":"310da51d-666e-4ab2-a552-d6826358161b"},"source":["### Choosing Specific GPUs\n"]},{"cell_type":"markdown","metadata":{"id":"9b427612-282d-487b-bebf-b338bf8b3c8e"},"source":["In order to specify a particular GPU to run on, we have to first check what units there are in the environment. The following lists out the information of each device, including the device name, type, memory limit, and so on.\n"]},{"cell_type":"code","source":["print(device_lib.list_local_devices())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jj5WhbHQAVvs","executionInfo":{"status":"ok","timestamp":1765550688522,"user_tz":-480,"elapsed":9,"user":{"displayName":"Htet Aung Lynn","userId":"02450750355632558897"}},"outputId":"aa9c7a7d-b747-42d9-87d4-fd6d09c850d0"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["[name: \"/device:CPU:0\"\n","device_type: \"CPU\"\n","memory_limit: 268435456\n","locality {\n","}\n","incarnation: 1286191349066891396\n","xla_global_id: -1\n",", name: \"/device:GPU:0\"\n","device_type: \"GPU\"\n","memory_limit: 14619377664\n","locality {\n","  bus_id: 1\n","  links {\n","  }\n","}\n","incarnation: 3205441879101513607\n","physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n","xla_global_id: 416903419\n","]\n"]}]},{"cell_type":"markdown","metadata":{"id":"30395535-d226-4faf-83df-f96e611b9f94"},"source":["To specify using a specific GPU, again use `tf.device()` with the `name` as input, just like we did for the CPU case. In the `with` statement, proceed with writing code as usual. Here, we are specifying `tensorflow` to be run on GPU ennumerated #2. We also use `%%timeit` here so you can compare the time that GPU took to run in comparison with CPU!\n"]},{"cell_type":"code","source":["%%timeit -n1 -r1\n","with tf.device('/device:GPU:0'):\n","    model_gpu = Sequential()\n","    model_gpu.add(Conv2D(input_shape = (28, 28, 1),\n","                     filters=5,\n","                     padding='Same',\n","                     kernel_size=(3,3)\n","                     ))\n","    model_gpu.add(MaxPooling2D(pool_size=(2,2)))\n","    model_gpu.add(Flatten())\n","    model_gpu.add(Dense(256, activation='relu'))\n","    model_gpu.add(Dense(10, activation='softmax'))\n","\n","    model_gpu.compile(optimizer='adam',\n","              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","\n","    model_gpu.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V4IbS6RdAknD","executionInfo":{"status":"ok","timestamp":1765550761039,"user_tz":-480,"elapsed":30918,"user":{"displayName":"Htet Aung Lynn","userId":"02450750355632558897"}},"outputId":"d7e3cafb-7f83-4239-9550-01512d800fc4"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.8766 - loss: 3.2588 - val_accuracy: 0.9468 - val_loss: 0.2131\n","Epoch 2/5\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9648 - loss: 0.1284 - val_accuracy: 0.9618 - val_loss: 0.1570\n","Epoch 3/5\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9736 - loss: 0.0939 - val_accuracy: 0.9641 - val_loss: 0.1484\n","Epoch 4/5\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9760 - loss: 0.0855 - val_accuracy: 0.9676 - val_loss: 0.1471\n","Epoch 5/5\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9785 - loss: 0.0767 - val_accuracy: 0.9652 - val_loss: 0.1489\n","30.9 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}]},{"cell_type":"markdown","metadata":{"id":"bf2bdc70-fdea-4e6f-9a6f-e72de762d0e4"},"source":["## Using CPU and GPU jointly\n"]},{"cell_type":"markdown","metadata":{"id":"d351769e-b0ee-4935-8460-d88958dcf117"},"source":["What if we want to use _both_ CPU and GPU for different parts of the same python script? Turns out we can do that too! Simply take advantage of the `tf.device()` function again to specify which unit the code fragment should be run on. Below, we show an example of how to run the same matrix operation on multiple GPUs and add up the tensors on CPU.\n"]},{"cell_type":"code","source":["# Enable tensor allocations or operations to be printed\n","tf.debugging.set_log_device_placement(True)\n","\n","# Get list of all logical GPUs\n","gpus = tf.config.list_logical_devices('GPU')\n","print(gpus)\n","\n","# Check if there are GPUs on this computer\n","if gpus:\n","  # Run matrix computation on multiple GPUs\n","    c = []\n","    for gpu in gpus:\n","        with tf.device(gpu.name):\n","            a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n","            b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n","            c.append(tf.matmul(a, b))\n","\n","    # Run on CPU\n","    with tf.device('/CPU:0'):\n","        matmul_sum = tf.add_n(c)\n","\n","    print(matmul_sum)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SokmaHgaBzJc","executionInfo":{"status":"ok","timestamp":1765550862785,"user_tz":-480,"elapsed":5,"user":{"displayName":"Htet Aung Lynn","userId":"02450750355632558897"}},"outputId":"b8b51795-eead-4325-c488-5448f803553d"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n","tf.Tensor(\n","[[22. 28.]\n"," [49. 64.]], shape=(2, 2), dtype=float32)\n"]}]},{"cell_type":"markdown","metadata":{"id":"6f5e08a4-84ed-47a8-be91-94f150505e81"},"source":["## Authors\n"]},{"cell_type":"markdown","metadata":{"id":"05568188-b276-45c8-bae8-4830046dad73"},"source":["[Cindy Huang](https://www.linkedin.com/in/cindy-shih-ting-huang/) is a data science associate of the Skills Network team. She has a passion for machine learning to improve user experience, especially in the area of computational linguistics.\n"]},{"cell_type":"markdown","metadata":{"id":"6163498b-9918-4e3a-9913-6e9d7335d98d"},"source":["### Other Contributors\n"]},{"cell_type":"markdown","metadata":{"id":"543eaeaf-3cfb-4a70-bbca-aa68a328f10c"},"source":["[Joseph Santarcangelo](https://www.linkedin.com/in/joseph-s-50398b136/) has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"]},{"cell_type":"markdown","metadata":{"id":"4bed1b07-548f-4003-819d-2c41a4232252"},"source":["## Change Log\n"]},{"cell_type":"markdown","metadata":{"id":"2e4d9f83-b39e-426b-82c2-bf34ec18a0ea"},"source":["|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n","|-|-|-|-|\n","|2022-07-11|0.1|Cindy H.|Created Lab|\n","|2022-07-21|0.2|Joseph S.|Reviewed Lab|\n","|2022-08-09|0.3|Steve H.|QA Pass|\n"]},{"cell_type":"markdown","metadata":{"id":"307697cd-80ac-4dcf-ad8e-2a96a12651ca"},"source":["Copyright © 2022 IBM Corporation. All rights reserved.\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}