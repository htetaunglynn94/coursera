











# Import basic libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Import sklearn libraries
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix

# Import imblearn libraries
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler


def evaluation_metrics(test, pred):
    accuracy = accuracy_score(test, pred)
    precision, recall, fbeta, _ = precision_recall_fscore_support(test, 
                                                                  pred, 
                                                                  beta = 5, 
                                                                  pos_label = 1,      # default
                                                                  average = 'binary') # default
    
    return [precision, recall, fbeta, accuracy]  


def evaluation_plot(test, pred):
    fig, axes = plt.subplots(1, 2, figsize=(10, 5))

    # Pie chart for test data distribution
    test_unique, test_counts = np.unique(test, return_counts=True)
    axes[0].pie(test_counts, 
                autopct = '%1.2f%%', 
                labels = ['matched', 'unmatched'], 
                colors = ['orange', 'lightgreen'],
                pctdistance = 0.6,
                labeldistance = 1.1,
                shadow = False,
                startangle = 90,
                textprops = {'size': 10})
    axes[0].set_title("Tested and Predicted Data Distribution")

    # Confusion matrix
    cf = confusion_matrix(test, pred, labels=[1,0])
    cfm = ConfusionMatrixDisplay(confusion_matrix=cf, display_labels=['True', 'False'])
    cfm.plot(ax=axes[1], cmap='Blues', colorbar=True)  # Plot into specific subplot
    axes[1].set_title("Confusion Matrix")
    
    plt.suptitle("Metrics Evaluation", fontsize=14)
    plt.tight_layout()
    plt.show()


def resample(X_train, y_train):

    # SMOTE sampler (Oversampling)
    smote_sampler = SMOTE(random_state = 123)
    # Undersampling
    under_sampler = RandomUnderSampler(random_state = 123)
    # Resampled datasets
    X_smo, y_smo = smote_sampler.fit_resample(X_train, y_train)
    X_under, y_under = under_sampler.fit_resample(X_train, y_train)

    print("Original \t:", np.unique(y_train, return_counts=True))
    print("SMOTE \t\t:", np.unique(y_smo, return_counts=True))
    print("UnderSampler \t:", np.unique(y_under, return_counts=True))

    return X_smo, y_smo, X_under, y_under


def plot_data_balancing(data, title="Data balance (Original)"):
    ax = data.value_counts().plot.bar(color=['green','blue'])
    for p in ax.patches:
        width = p.get_width()
        height = p.get_height()
        
        xy = (p.get_x() + width/2, height)
        ax.annotate(f"{height} times",xy, va='bottom', ha='center', color='blue')
        ax.set_title(
            title)
        ax.set_xticklabels(['No','Yes'])
        ax.set_ylabel("Counts")      


rs = 123

# LINEAR REGRESSION
def find_best_params_LR(X_train, y_train):
    parameters = {'class_weight': [{0:0.05, 1:0.95}, {0:0.1, 1:0.9}, {0:0.2, 1:0.8}]}
    lr = LogisticRegression(random_state=rs, max_iter=1000)
    gs = GridSearchCV(estimator = lr, 
                      param_grid = parameters, 
                      scoring = 'f1', 
                      cv = 5, 
                      verbose = 1)
    gs.fit(X_train, y_train.values.ravel())
    best_params = gs.best_params_
    return gs, best_params

# DECISION TREE
def find_best_params_DT(X_train, y_train):
    parameters = {'max_depth': [5,10,15,20], 
                  'class_weight': [{0:0.1, 1:0.9}, {0:0.2, 1:0.8}, {0:0.3, 1:0.7}],
                  'min_samples_split': [2,5]}
    dtc = DecisionTreeClassifier(random_state=rs)
    gs  = GridSearchCV(estimator = dtc, 
                       param_grid = parameters, 
                       scoring='f1', 
                       cv = 5, 
                       verbose = 1)
    gs.fit(X_train, y_train.values.ravel())
    best_params = gs.best_params_
    return gs, best_params

# RANDOMFOREST 
def find_best_params_RF(X_train, y_train):
    parameters = {'max_depth': [5, 10, 15, 20],
                  'n_estimators': [25, 50, 100],
                  'min_samples_split': [2, 5],
                  'class_weight': [{0:0.1, 1:0.9}, {0:0.2, 1:0.8}, {0:0.3, 1:0.7}]}
    rf = RandomForestClassifier(random_state=rs)
    gs = GridSearchCV(estimator = rf, 
                      param_grid = parameters, 
                      scoring = 'f1', 
                      cv = 5, 
                      verbose = 1)
    gs.fit(X_train, y_train.values.ravel())
    best_params = gs.best_params_
    return gs, best_params

# XGBoost
def find_best_params_XGB(X_train, y_train):
    parameters = {'n_estimators': [25,50,100],
                  'max_depth': [5,10,15,20],
                  'learning_rate': [0.01, 0.1, 0.2]}

    xgb = XGBClassifier(random_state=rs)
    gs = GridSearchCV(estimator = rf, 
                      param_grid = parameters, 
                      scoring = 'f1', 
                      cv = 5, 
                      verbose = 1)
    gs.fit(X_train, y_train.values.ravel())
    best_params = gs.best_params_
    return gs, best_params


def train_models(X_train, X_test, y_train, y_test):
    LR, lr_params = find_best_params_LR(X_train, y_train)
    DTC, dtc_params = find_best_params_DT(X_train, y_train)
    RFC, rf_params = find_best_params_RF(X_train, y_train)
    
    pred_lr = LR.predict(X_test)
    pred_dtc = DTC.predict(X_test)
    pred_rfc = RFC.predict(X_test)

    lr = evaluation_metrics(y_test, pred_lr)
    dtc = evaluation_metrics(y_test, pred_dtc)
    rfc = evaluation_metrics(y_test, pred_rfc)

    # Linear Regression evaluation scores
    # FORMAT --> [precision, recall, fbeta, accuracy]
    print('Logistic Regression')
    print('-' * 25)
    print(f'Best parameters\t: {lr_params}')
    print(f"Precision\t: {lr[0]:.2f}")
    print(f"Recall\t\t: {lr[1]:.2f}")
    print(f"F-score\t\t: {lr[2]:.2f}")
    print(f"Accuracy\t: {lr[3]:.2f}")
    print('-' * 25)

    # Decision Tree evaluation scores
    print('Decision Tree')
    print('-' * 25)
    print(f'Best parameters\t: {dtc_params}')
    print(f"Precision\t: {dtc[0]:.2f}")
    print(f"Recall\t\t: {dtc[1]:.2f}")
    print(f"F-score\t\t: {dtc[2]:.2f}")
    print(f"Accuracy\t: {dtc[3]:.2f}")
    print('-' * 25)
    
    # Random Forest evaluation scores
    print('Random Forest')
    print('-' * 25)
    print(f'Best parameters\t: {rf_params}')
    print(f"Precision\t: {rfc[0]:.2f}")
    print(f"Recall\t\t: {rfc[1]:.2f}")
    print(f"F-score\t\t: {rfc[2]:.2f}")
    print(f"Accuracy\t: {rfc[3]:.2f}")

    return pred_lr, pred_dtc, pred_rfc


def resample(train1, train2):
    train1_smt, train2_smt = SMOTE(random_state = 123).fit_resample(train1, train2)
    train1_rus, train2_rus = RandomUnderSampler(random_state = 123).fit_resample(train1, train2)
    print(f'Original\t : {np.unique(train2, return_counts=True)}')
    print(f'Up-sample \t : {np.unique(train2_smt, return_counts=True)}')
    print(f'Down-sample\t : {np.unique(train2_rus, return_counts=True)}')

    return train1_smt, train2_smt, train1_rus, train2_rus


df = pd.read_csv('im_churn.csv', index_col=False)











df.head()


df.info()


df.shape


df.shape[0] * df.shape[1]


plot_data_balancing(df['Class'])





X = df.loc[:, df.columns != 'Class']
y = df[['Class']]


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=123)


ss = StandardScaler()
X_train_s = ss.fit_transform(X_train)
X_test_s  = ss.transform(X_test)


np.unique(y_train, return_counts=True)





pred_lr, pred_dtc, pred_rfc = train_models(X_train_s, X_test_s, y_train, y_test)


evaluation_plot( y_test, pred_lr)


evaluation_plot(y_test, pred_dtc)


evaluation_plot( y_test, pred_rfc)





X_train_sm, y_train_sm, X_train_under, y_train_under = resample(X_train_s, y_train)


plot_data_balancing(y_train_sm, "Data balance (after upsampling)")


plot_data_balancing(y_train_under, "Data balance (after downsampling)")





pred_lr, pred_dtc, pred_rfc = train_models(X_train_sm, X_test_s, y_train_sm, y_test)


evaluation_plot( y_test, pred_lr)


evaluation_plot(y_test, pred_dtc)


evaluation_plot(y_test, pred_rfc)





pred_lr, pred_dtc, pred_rfc = train_models(X_train_under, X_test_s, y_train_under, y_test)


evaluation_plot( y_test, pred_lr)


evaluation_plot( y_test, pred_dtc)


evaluation_plot( y_test, pred_rfc)



