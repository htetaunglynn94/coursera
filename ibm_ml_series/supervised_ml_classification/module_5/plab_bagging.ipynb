{"cells":[{"cell_type":"markdown","id":"c23cf365-f345-4b2b-beb6-f48f7c362e45","metadata":{},"outputs":[],"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n","    </a>\n","</p>\n","\n","# Bootstrap Aggregating (Bagging) for classification with Python\n","\n","Estimated time needed: **45** minutes\n","\n","## Objectives\n","\n","After completing this lab you will be able to:\n","\n","*   Understand Bootstrap sampling\n","*   Understand  Model Instability\n","*   Apply Bagging\n","*   Understand when to use Bagging\n"]},{"cell_type":"markdown","id":"b98e76a6-15e2-499c-a7ed-2518127328f1","metadata":{},"outputs":[],"source":["In this notebook, you will learn the process of Bagging  (Bootstrap Aggregation)  models for classification. Bagging is a method for generating multiple model versions and aggregating the ensemble of models to make a single prediction. For classification, aggregation performs majority vote when predicting a class. The various versions of the model are formed by performing Bootstrap sampling of the training set and using these to train each model in the ensemble .\n"]},{"cell_type":"markdown","id":"34eca2a5-745c-4ce9-b9f7-50d9a45b009e","metadata":{},"outputs":[],"source":["<h1>Table of contents</h1>\n","\n","<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n","    <ol>\n","        <li><a href=\"https://#about_dataset\">About the dataset</a></li>\n","        <li><a href=\"https://#preprocessing\">Data pre-processing and selection</a></li>\n","        <li><a href=\"https://#modeling\">Modeling (Logistic Regression with Scikit-learn)</a></li>\n","        <li><a href=\"https://#evaluation\">Evaluation</a></li>\n","        <li><a href=\"https://#practice\">Practice</a></li>\n","    </ol>\n","</div>\n","<br>\n","<hr>\n"]},{"cell_type":"markdown","id":"b781ccdb-ce9d-436c-b171-04642a80da40","metadata":{},"outputs":[],"source":["Let's first import required libraries:\n"]},{"cell_type":"code","id":"c3124148-a068-47bc-b0c8-5918b7114c40","metadata":{},"outputs":[],"source":["# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n# !mamba install -qy pandas==1.3.3 numpy==1.21.2 ipywidgets==7.4.2 scipy==7.4.2 tqdm==4.62.3 matplotlib==3.5.0 seaborn==0.9.0\n# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\" "]},{"cell_type":"code","id":"2c90a21c-65df-46f7-b3d3-2a87b58eb64c","metadata":{},"outputs":[],"source":["# Library for reading in data and using dataframes\nimport pandas as pd\n# Using numpy arrays\nimport numpy as np\n# Data preprocessing functions like LabelEncoder\nfrom sklearn import preprocessing\n%matplotlib inline\n# Visualizations\nimport matplotlib.pyplot as plt\n# Model accuracy\nfrom sklearn import metrics\n# Surpress numpy data type warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n# Give loops a progress bar\nfrom tqdm import tqdm"]},{"cell_type":"markdown","id":"5d8ac2d3-8de5-4215-875f-824a96cd9be5","metadata":{},"outputs":[],"source":["This function calculates the train and test accuracy of a model\n"]},{"cell_type":"code","id":"629af03f-14f1-4301-8f02-c6508590a215","metadata":{},"outputs":[],"source":["def get_accuracy(X_train, X_test, y_train, y_test, model):\n    return  {\"test Accuracy\":metrics.accuracy_score(y_test, model.predict(X_test)),\"train Accuracy\": metrics.accuracy_score(y_train, model.predict(X_train))}"]},{"cell_type":"markdown","id":"a72fb6c7-979f-4d5d-bf91-0162caa53b7a","metadata":{},"outputs":[],"source":["This function creates visualizations of decision trees\n"]},{"cell_type":"code","id":"885d7b1a-a234-4512-8938-48dcff35fd9a","metadata":{},"outputs":[],"source":["# Plot tree helper libraries\nfrom  io import StringIO\nimport pydotplus\nimport matplotlib.image as mpimg\nfrom sklearn import tree\n\n\ndef plot_tree(model,filename = \"tree.png\"):\n    #global churn_df \n\n    dot_data = StringIO()\n  \n\n    featureNames = [colunm  for colunm in churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip']].columns]\n    out=tree.export_graphviz(model,feature_names=featureNames, out_file=dot_data, class_names= ['left','stay'], filled=True,  special_characters=True,rotate=False)  \n    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n    graph.write_png(filename)\n    img = mpimg.imread(filename)\n    plt.figure(figsize=(100, 200))\n    plt.imshow(img,interpolation='nearest')"]},{"cell_type":"markdown","id":"9bfe47f1-89c2-4ca3-ae93-8e3ede72bf9a","metadata":{},"outputs":[],"source":["This function creates a graph of training accuracy vs how many estimators (Decision Trees) a BaggingClassifier uses\n"]},{"cell_type":"code","id":"8ecec4a1-706a-4852-b845-7453fd1de902","metadata":{},"outputs":[],"source":["def get_accuracy_bag(X,y,title,times=20,xlabel='Number Estimators'):\n    #Iterate through different number of estimators and average out the results  \n\n\n    N_estimators=[n for n in range(1,70)]\n    times=20\n    train_acc=np.zeros((times,len(N_estimators)))\n    test_acc=np.zeros((times,len(N_estimators)))\n    \n    train_time=np.zeros((times,len(N_estimators)))\n    test_time=np.zeros((times,len(N_estimators)))\n     #average out the results\n    for n in tqdm(range(times)):\n        X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3)\n        for n_estimators in N_estimators:\n            #Iterate through different number of estimators and average out the results   \n        \n            Bag= BaggingClassifier(base_estimator=DecisionTreeClassifier(criterion=\"entropy\", max_depth = 10),n_estimators=n_estimators,bootstrap=True,random_state=0)\n            Bag.fit(X_train,y_train)\n          \n            \n             \n            Accuracy=get_accuracy(X_train, X_test, y_train, y_test,  Bag)\n           \n            \n            \n  \n            train_acc[n,n_estimators-1]=Accuracy['train Accuracy']\n            test_acc[n,n_estimators-1]=Accuracy['test Accuracy']\n        \n        \n        \n    fig, ax1 = plt.subplots()\n\n    ax2 = ax1.twinx()\n    ax1.plot(train_acc.mean(axis=0))\n    ax2.plot(test_acc.mean(axis=0),c='r')\n    ax1.set_xlabel(xlabel)\n    ax1.set_ylabel('Training accuracy',color='b')\n    ax2.set_ylabel('Testing accuracy', color='r')\n    plt.title(title)\n    plt.show()\n    \n   "]},{"cell_type":"markdown","id":"e9897a12-ee66-41d7-8fe9-49a416239268","metadata":{},"outputs":[],"source":["## Customer churn\n","\n","A telecommunications company is concerned about the number of customers leaving their land-line business for cable competitors. They need to understand who is leaving. Imagine that you are an analyst at this company and you have to find out why\n"]},{"cell_type":"markdown","id":"4176d9ab-5ded-4b65-95f1-318e098d6eb0","metadata":{},"outputs":[],"source":["### About the dataset\n","\n","We will use a telecommunications dataset for predicting customer churn. This is a historical customer dataset where each row represents one customer. The data is relatively easy to understand, and you may uncover insights you can use immediately. Typically it is less expensive to keep customers than acquire new ones, so the focus of this analysis is to predict the customers who will stay with the company.\n","\n","This data set provides information to help you predict what behavior will help you to retain customers. You can analyze all relevant customer data and develop focused customer retention programs.\n","\n","The dataset includes information about:\n","\n","*   Customers who left within the last month – the column is called Churn\n","*   Services that each customer has signed up for – phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies\n","*   Customer account information – how long they had been a customer, contract, payment method, paperless billing, monthly charges, and total charges\n","*   Demographic info about customers – gender, age range, and if they have partners and dependents\n"]},{"cell_type":"markdown","id":"198a0603-8704-418c-a880-222ae6df7c6c","metadata":{},"outputs":[],"source":["### Load Data From CSV File\n"]},{"cell_type":"code","id":"de6dc545-68d0-4234-8792-7415e778b3b8","metadata":{},"outputs":[],"source":["churn_df = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/data/ChurnData.csv\")\n\nchurn_df.head()"]},{"cell_type":"markdown","id":"9db5bfc8-eedf-4cdb-8246-a527f5786006","metadata":{},"outputs":[],"source":["<h2 id=\"preprocessing\">Data pre-processing and selection</h2>\n"]},{"cell_type":"markdown","id":"439286fe-a284-4d94-b00d-edef3ca7c82a","metadata":{},"outputs":[],"source":["Let's select some features for the modeling. Also, we change the target data type to be an integer, as it is a requirement by the skitlearn algorithm:\n"]},{"cell_type":"code","id":"e53ae1ee-1bf5-46a3-b96d-7a8b1db0c167","metadata":{},"outputs":[],"source":["churn_df = churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip',   'callcard', 'wireless','churn']]\nchurn_df['churn'] = churn_df['churn'].astype('int')\nchurn_df.head()"]},{"cell_type":"markdown","id":"a64e77bb-9688-4afb-b173-31f011290469","metadata":{},"outputs":[],"source":["## Practice\n","\n","How many rows and columns are in this dataset in total? What are the names of columns?\n"]},{"cell_type":"code","id":"f26e6ad0-a385-45b4-8030-231780f8bc84","metadata":{},"outputs":[],"source":["# write your code here\n"]},{"cell_type":"markdown","id":"080e12cc-dfb5-4b3a-b501-b7b1cbe89099","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","\n","```python\n","print(churn_df.shape)\n","\n","print(churn_df.columns)\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"74835eff-709c-4ba6-94d8-d16b1c9850ef","metadata":{},"outputs":[],"source":["## Bootstrap Sampling\n","\n","Bootstrap Sampling is a method that involves drawing of sample data repeatedly with replacement from a data source to estimate a model parameter. Scikit-learn has methods for Bagging but its helpful to understand Bootstrap sampling.  We will import <code>resample</code>\n"]},{"cell_type":"code","id":"51cd521b-5c53-43d9-a194-29b5b94ef06a","metadata":{},"outputs":[],"source":["from sklearn.utils import resample"]},{"cell_type":"markdown","id":"54ed5188-9bcf-4d3b-a376-1b26a660076f","metadata":{},"outputs":[],"source":["Consider the five rows of data:\n"]},{"cell_type":"code","id":"fefbc30a-7e6e-4a1f-8011-a830a94292b1","metadata":{},"outputs":[],"source":["churn_df[0:5]"]},{"cell_type":"markdown","id":"c3d47725-7bee-48c1-bc70-f0a5bf143029","metadata":{},"outputs":[],"source":["We can perform a bootstrap sample using the function <code>resample</code>; we see the dataset is the same size, but some rows are repeated:\n"]},{"cell_type":"code","id":"cd4b8278-86de-4d66-b24f-cd1fa33a847f","metadata":{},"outputs":[],"source":["resample(churn_df[0:5])"]},{"cell_type":"markdown","id":"2e97e4bf-eefc-40bd-9082-e4a0bcd8b27f","metadata":{},"outputs":[],"source":["We can repeat the process randomly drawing several other rows\n"]},{"cell_type":"code","id":"234c8f1b-6ddf-4c9a-ad32-c681ba079dbe","metadata":{},"outputs":[],"source":["resample(churn_df[0:5])"]},{"cell_type":"markdown","id":"623a224c-569d-4081-8608-90216001f29e","metadata":{},"outputs":[],"source":["## Train/Test dataset\n"]},{"cell_type":"markdown","id":"2131769b-5c20-42e3-9079-09912aca6d9e","metadata":{},"outputs":[],"source":["Let's define X, and y for our dataset:\n"]},{"cell_type":"code","id":"491330c1-23c1-49ff-83ec-d0d5ea5caa5d","metadata":{},"outputs":[],"source":["X = churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip']]\n\nX.head()"]},{"cell_type":"code","id":"a6c1ec6a-fca4-494c-a71d-547756d0e6e3","metadata":{},"outputs":[],"source":["y = churn_df['churn']\ny.head()"]},{"cell_type":"markdown","id":"27293acf-8f0e-4fb9-ae9a-8b6e95c22011","metadata":{},"outputs":[],"source":["## Train/Test dataset\n"]},{"cell_type":"markdown","id":"e1abcd42-54ef-4831-8d84-3fc60d998078","metadata":{},"outputs":[],"source":["We split our dataset into train and test set:\n"]},{"cell_type":"code","id":"dd9730ec-7d68-4fa6-b63c-c23993e82ae8","metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=0)\nprint ('Train set', X_train.shape,  y_train.shape)\nprint ('Test set', X_test.shape,  y_test.shape)"]},{"cell_type":"markdown","id":"79de8666-faff-465f-a3a2-e51383423466","metadata":{},"outputs":[],"source":["<h2 id=\"modeling\">Decision  Tree Classifier  with Scikit-learn</h2>\n"]},{"cell_type":"markdown","id":"0c0efea5-97bd-4b73-a6c2-ab16f64ff7d2","metadata":{},"outputs":[],"source":["A Decision  tree Classifier classifies a sample by learning simple decision rules inferred from the data. One problem with Decision  Tree Classifiers is overfitting; they do well with the training data, but they do not Generalize well. Trees have low bias and high variance; as such, they are a prime candidate for Bagging. Instability is another term used to describe models that overfit. Instability is characterized by a slight change in the training set that causes a drastic change in the model.  Let's show that Decision tree Classifiers are unstable.\n"]},{"cell_type":"markdown","id":"43c42964-19cf-4be1-bd9b-63c27900ac5e","metadata":{},"outputs":[],"source":["Let's load the DecisionTreeClassifier modle in   <coode>sklearn</code>\n"]},{"cell_type":"code","id":"4a623056-a46c-410e-b3a0-31e2b9aee7a1","metadata":{},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree"]},{"cell_type":"markdown","id":"f580e5a2-d84d-471a-affd-685a0f08154f","metadata":{},"outputs":[],"source":["We create and train a tree with a max depth of  5\n"]},{"cell_type":"code","id":"960057b1-ff37-41fc-9513-bf3e9d4299e3","metadata":{},"outputs":[],"source":["max_depth=5\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=10)\nTree = DecisionTreeClassifier(criterion=\"entropy\", max_depth = max_depth,random_state=10)\nTree\nTree.fit(X_train,y_train)"]},{"cell_type":"markdown","id":"a9b6bba0-7d38-4af0-a120-abaa25191723","metadata":{},"outputs":[],"source":["Now we can predict using our test set:\n"]},{"cell_type":"code","id":"0f2d8d30-7ae9-45fc-b090-f241e128cef7","metadata":{},"outputs":[],"source":["yhat = Tree.predict(X_test)\nyhat"]},{"cell_type":"markdown","id":"ac400d96-6818-451d-83b7-6474adc7f2ec","metadata":{},"outputs":[],"source":["We see the test error is much larger than the training error:\n"]},{"cell_type":"code","id":"40b9ed64-8a56-4820-9e80-1585738c81c4","metadata":{},"outputs":[],"source":["get_accuracy(X_train, X_test, y_train, y_test,  Tree)"]},{"cell_type":"markdown","id":"cc97d543-fc8d-430b-b914-b404237e108c","metadata":{},"outputs":[],"source":["We can plot the nodes of the tree:\n"]},{"cell_type":"code","id":"a2e07702-cfcf-4ce1-93a6-cf058e66a943","metadata":{},"outputs":[],"source":["plot_tree(filename = \"tree.png\",model=Tree)"]},{"cell_type":"markdown","id":"4b0375d4-8328-4bc4-9caf-bc09ce4312e0","metadata":{},"outputs":[],"source":["We can repeat the process but sampling different data points from the same dataset.  We see the tree still suffers from overfitting; in addition, the new tree is entirely different.\n"]},{"cell_type":"code","id":"58d54980-ea28-445c-bf53-00d71d9d3224","metadata":{},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=5)\nTree = DecisionTreeClassifier(criterion=\"entropy\", max_depth = max_depth,random_state=8)\nTree.fit(X_train,y_train)\nprint(get_accuracy(X_train, X_test, y_train, y_test,  Tree))\nplot_tree(filename = \"tree1.png\",model=Tree)"]},{"cell_type":"markdown","id":"17beb537-6579-47f0-a599-bd0ca2f99468","metadata":{},"outputs":[],"source":["<h2 id=\"evaluation\">Bagging  for classification with  Scikit-learn</h2>\n","A Bagging classifier is an ensemble model that trains  base classifiers  on random subsets   of the original dataset  (Bootstrap Sampling by default), and then aggregate their individual predictions by voting. We import the module:\n"]},{"cell_type":"code","id":"fa74f082-0b6c-4cc5-9e21-d320aab812ae","metadata":{},"outputs":[],"source":["from sklearn.ensemble import BaggingClassifier"]},{"cell_type":"markdown","id":"17dd2a64-9147-4309-96e9-478a9346f9be","metadata":{},"outputs":[],"source":["Bagging improves models that suffer from overfitting; they do well on the training data, but they do not Generalize well. Decision Trees are a prime candidate for this reason, in addition, they are fast to train; We create a <code>BaggingClassifier</code> object,  with a Decision Tree as the <code>base_estimator</code>\n"]},{"cell_type":"code","id":"7a0800e6-4724-404e-8774-bc3c7bb78f80","metadata":{},"outputs":[],"source":["Bag = BaggingClassifier(base_estimator=DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4,random_state=2),n_estimators=30,random_state=0,bootstrap=True)"]},{"cell_type":"markdown","id":"9e76a741-4217-4c1d-8185-bb3aa929df98","metadata":{},"outputs":[],"source":["We fit the model:\n"]},{"cell_type":"code","id":"97122600-7b90-4a80-9603-114b5314d35b","metadata":{},"outputs":[],"source":["Bag.fit(X_train,y_train)"]},{"cell_type":"markdown","id":"71d218f3-ad1e-44b3-b134-a8a97568a20d","metadata":{},"outputs":[],"source":["The method <code>predict</code>  aggregates the predictions by voting:\n"]},{"cell_type":"code","id":"62b46ba1-dbf3-4a6e-9ad8-46090bcb2bb0","metadata":{},"outputs":[],"source":["Bag.predict(X_test)"]},{"cell_type":"markdown","id":"43bafd47-12bb-4a5f-8a99-ad4231765466","metadata":{},"outputs":[],"source":["We see the training accuracy is slightly better but the test accuracy improves  dramatically:\n"]},{"cell_type":"code","id":"8121bc15-c204-4c63-b61c-b8c4d419e5ea","metadata":{},"outputs":[],"source":["print(get_accuracy(X_train, X_test, y_train, y_test,  Bag))"]},{"cell_type":"markdown","id":"9c4bb039-1036-4f6d-a939-f8ad192a969e","metadata":{},"outputs":[],"source":["Here we can see the impact of adding more estimators (Decision Trees) using in Bagging on the testing and training accuracy\n"]},{"cell_type":"code","id":"937d7a7f-2739-4d34-9f83-78449c129f49","metadata":{},"outputs":[],"source":["get_accuracy_bag(X, y, \"Customer Churn\")"]},{"cell_type":"markdown","id":"aab3b72c-7ba8-46bd-a36a-02937f2e46f8","metadata":{},"outputs":[],"source":["## Low Variance Example\n"]},{"cell_type":"markdown","id":"dc89eb36-dbe1-4ff3-bc5b-7714f13c260e","metadata":{},"outputs":[],"source":["Bagging does not improve result if the model has low Variance i.e. does reasonably well on the test and training data. Consider the SVM; the accuracy on the tests data and training data are similar\n"]},{"cell_type":"code","id":"29ec7666-fd49-4fd5-a418-f6a63083afaa","metadata":{},"outputs":[],"source":["from sklearn.svm import SVC\n\nclf=SVC(kernel='linear',gamma='scale')\nclf.fit(X_train, y_train) \nprint(get_accuracy(X_train, X_test, y_train, y_test,  clf))"]},{"cell_type":"markdown","id":"56dfdd84-1e25-4ab4-af91-d4d8b5fc0959","metadata":{},"outputs":[],"source":["Bagging the SVM does almost nothing:\n"]},{"cell_type":"code","id":"5ae68c59-b6c6-4f40-8d0c-d8614ad34d8c","metadata":{},"outputs":[],"source":["Bag = BaggingClassifier(base_estimator=SVC(kernel='linear',gamma='scale'),n_estimators=10,random_state=0,bootstrap=True)\nBag.fit(X_train,y_train)\nprint(get_accuracy(X_train, X_test, y_train, y_test,  Bag))"]},{"cell_type":"markdown","id":"3683484e-609b-4a70-8f54-0266d8746081","metadata":{},"outputs":[],"source":["<h2 id=\"practice\">Practice: Cancer data</h2>\n","\n","The example is based on a dataset that is publicly available from the UCI Machine Learning Repository (Asuncion and Newman, 2007)[[http://mlearn.ics.uci.edu/MLRepository.html](http://mlearn.ics.uci.edu/MLRepository.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML241ENSkillsNetwork31576874-2022-01-01)]. The dataset consists of several hundred human cell sample records, each of which contains the values of a set of cell characteristics. The fields in each record are:\n","\n","| Field name  | Description                 |\n","| ----------- | --------------------------- |\n","| ID          | Clump thickness             |\n","| Clump       | Clump thickness             |\n","| UnifSize    | Uniformity of cell size     |\n","| UnifShape   | Uniformity of cell shape    |\n","| MargAdh     | Marginal adhesion           |\n","| SingEpiSize | Single epithelial cell size |\n","| BareNuc     | Bare nuclei                 |\n","| BlandChrom  | Bland chromatin             |\n","| NormNucl    | Normal nucleoli             |\n","| Mit         | Mitoses                     |\n","| Class       | Benign or malignant         |\n","\n","<br>\n","<br>\n","\n","Let's load the dataset:\n"]},{"cell_type":"code","id":"29647478-0d00-47af-826b-40da2c16bf94","metadata":{},"outputs":[],"source":["df = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/data/cell_samples.csv\")\n\ndf.head()"]},{"cell_type":"markdown","id":"03feb819-8314-4257-aceb-af71bf5dce5d","metadata":{},"outputs":[],"source":["Now lets remove rows that have a ? in the `BareNuc` column\n"]},{"cell_type":"code","id":"1c070f3f-91c3-4805-aed4-87bd46fd38a9","metadata":{},"outputs":[],"source":["df = df[df[\"BareNuc\"] != \"?\"]"]},{"cell_type":"markdown","id":"9c3acf58-4c74-4ea7-af3f-97c27f34f52f","metadata":{},"outputs":[],"source":["Now lets define the X and y for our dataset\n"]},{"cell_type":"code","id":"457a1135-2907-4d35-839c-fb748376b6bc","metadata":{},"outputs":[],"source":["X =  df[['Clump', 'UnifSize', 'UnifShape', 'MargAdh', 'SingEpiSize', 'BareNuc', 'BlandChrom', 'NormNucl', 'Mit']]\n\nX.head()"]},{"cell_type":"code","id":"072756de-4d09-4c59-9d2f-1eb505ff1908","metadata":{},"outputs":[],"source":["y = df['Class']\n\ny.head()"]},{"cell_type":"markdown","id":"70a2c67c-1a2c-4f13-9b39-960473c8ab6e","metadata":{},"outputs":[],"source":["Now lets split our data into training and testing data with a 80/20 split\n"]},{"cell_type":"code","id":"482e377c-c306-43a5-8106-e9fcf1570b27","metadata":{},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)"]},{"cell_type":"markdown","id":"f6e8e463-1684-40be-9ebb-ca642932ab63","metadata":{},"outputs":[],"source":["Now to determine the best parameters for `n_estimators` and the `max_depth` of the `base_estimator` we will use `GridSearchCV`\n"]},{"cell_type":"code","id":"3ffb8d20-802b-4ffb-91f7-0aec08bace1b","metadata":{},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV"]},{"cell_type":"markdown","id":"e0c19b78-fa67-4332-9778-2a778f09423b","metadata":{},"outputs":[],"source":["We can use GridSearch for Exhaustive search over specified parameter values. To  alter the base  model; we add the double underscore and the attribute value:\n","\n","Here we are searching odd numbers from 1 to 39 for `n_estimators` and odd numbers from 1 to 20 for `max_depth` in the `base_estimator`\n"]},{"cell_type":"code","id":"c1e86025-542f-4739-a34e-2e3db5636ac7","metadata":{},"outputs":[],"source":["param_grid = {'n_estimators': [2*n+1 for n in range(20)],\n     'base_estimator__max_depth' : [2*n+1 for n in range(10) ] }"]},{"cell_type":"markdown","id":"0683c95f-0a38-4803-995b-e0932b844250","metadata":{},"outputs":[],"source":["Create a `BaggingClassifier` object called `Bag` with the `base_estimator` set to a `DecisionTreeClassifier` object where `random_state` = 0 and `bootstrap` = True\n"]},{"cell_type":"code","id":"ed2cd580-850c-42e8-a0ee-cd97e4c3d5eb","metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","id":"4e5dd410-a0f0-4195-a679-21b90eb91822","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","\n","```python\n","Bag = BaggingClassifier(base_estimator = DecisionTreeClassifier(), random_state=0, bootstrap=True)\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"d7cfbff4-8a1a-45a1-bc68-2346311741ce","metadata":{},"outputs":[],"source":["Now we create a `GridSearchCV` object and search for the best parameters according to our `parameter_grid`\n"]},{"cell_type":"code","id":"0afa92e9-a68a-4e56-8cb8-70b9a50f4f52","metadata":{},"outputs":[],"source":["search = GridSearchCV(estimator=Bag, param_grid=param_grid, scoring='accuracy', cv=3)"]},{"cell_type":"code","id":"a0fb5500-6e35-4802-b6a2-6632e895e25a","metadata":{},"outputs":[],"source":["search.fit(X_train, y_train)"]},{"cell_type":"markdown","id":"9b96742d-b461-4156-81df-c5a8a99283ad","metadata":{},"outputs":[],"source":["We can see the best accuracy score of the searched parameters was ~97%\n"]},{"cell_type":"code","id":"7c4788ad-737c-44df-9216-3e43e2c7e80d","metadata":{},"outputs":[],"source":["search.best_score_"]},{"cell_type":"markdown","id":"1c6c115f-ea5a-46cb-9de1-5c2092c016fa","metadata":{},"outputs":[],"source":["We can also see the parameters that resulted in the best score\n"]},{"cell_type":"code","id":"7737b925-15e3-451a-8fe2-b618a60e7f0b","metadata":{},"outputs":[],"source":["search.best_params_"]},{"cell_type":"markdown","id":"ddd98e4c-c67a-4093-b79d-2e921341344a","metadata":{},"outputs":[],"source":["And we can see the testing and training accuracy of the best estimator\n"]},{"cell_type":"code","id":"e8e2e985-2059-43a3-ba90-050bbac9ae09","metadata":{},"outputs":[],"source":["print(get_accuracy(X_train, X_test, y_train, y_test, search.best_estimator_))"]},{"cell_type":"markdown","id":"e885f528-fa8f-432e-9a2b-19f6c786e0e5","metadata":{},"outputs":[],"source":["Below we can see a graph of testing and training accuracy holding the `max_depth` of the `base_estimator` at 10 and varying the number of estimators. We can see that it is extremely close to the accuracy of the `best_estimator` we found using `GridSearchCV`\n"]},{"cell_type":"code","id":"601c7fc1-270d-4549-9776-85af3433c6ae","metadata":{},"outputs":[],"source":["get_accuracy_bag(X, y, \"Cancer Data\")"]},{"cell_type":"markdown","id":"7ee84b9a-66fe-4c95-bef0-827f383c5fc1","metadata":{},"outputs":[],"source":["<h2 id=\"practice\">Practice: During their course of treatment</h2>\n"]},{"cell_type":"markdown","id":"c4baaaed-53b3-41f9-9b2e-b98977af9345","metadata":{},"outputs":[],"source":["Imagine that you are a medical researcher compiling data for a study. You have collected data about a set of patients, all of whom suffered from the same illness. During their course of treatment, each patient responded to one of 5 medications, Drug A, Drug B, Drug c, Drug x and y.\n","\n","Part of your job is to build a model to find out which drug might be appropriate for a future patient with the same illness. The features of this dataset are Age, Sex, Blood Pressure, and the Cholesterol of the patients, and the target is the drug that each patient responded to.\n","\n","It is a sample of multiclass classifier, and you can use the training part of the dataset to build a decision tree, and then use it to predict the class of a unknown patient, or to prescribe a drug to a new patient.\n"]},{"cell_type":"code","id":"67048070-c2d1-4281-848b-3c696a3eb900","metadata":{},"outputs":[],"source":["df = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/data/drug200.csv\", delimiter=\",\")\ndf.head()"]},{"cell_type":"markdown","id":"bb5e098a-1a22-45b1-81ea-43c0d114b57c","metadata":{},"outputs":[],"source":["Lets create the X and y for our dataset\n"]},{"cell_type":"code","id":"d8ee192a-e96d-4464-a512-07ac1c6f90c5","metadata":{},"outputs":[],"source":["X = df[['Age', 'Sex', 'BP', 'Cholesterol', 'Na_to_K']].values\nX[0:5]"]},{"cell_type":"code","id":"b65ba871-076f-49fd-944d-8e312fcda3e7","metadata":{},"outputs":[],"source":["y = df[\"Drug\"]\ny[0:5]"]},{"cell_type":"markdown","id":"78b09241-7cac-4798-b027-811ea525e3b3","metadata":{},"outputs":[],"source":["Now lets use a `LabelEncoder` to turn categorical features into numerical\n"]},{"cell_type":"code","id":"e97ef997-82de-4871-ac7f-2273be44e32a","metadata":{},"outputs":[],"source":["le_sex = preprocessing.LabelEncoder()\nle_sex.fit(['F','M'])\nX[:,1] = le_sex.transform(X[:,1]) "]},{"cell_type":"code","id":"44ad6540-1a69-4f8c-abae-9fca44318a36","metadata":{},"outputs":[],"source":["le_BP = preprocessing.LabelEncoder()\nle_BP.fit([ 'LOW', 'NORMAL', 'HIGH'])\nX[:,2] = le_BP.transform(X[:,2])"]},{"cell_type":"code","id":"bc64f980-301f-4746-8aec-c05d377f03a1","metadata":{},"outputs":[],"source":["le_Chol = preprocessing.LabelEncoder()\nle_Chol.fit([ 'NORMAL', 'HIGH'])\nX[:,3] = le_Chol.transform(X[:,3]) "]},{"cell_type":"code","id":"04df8856-b72b-4727-9be8-e84f0e7554e1","metadata":{},"outputs":[],"source":["X[0:5]"]},{"cell_type":"markdown","id":"d9ff5a03-6cca-4f9f-962f-07293bceeb26","metadata":{},"outputs":[],"source":["Split the data into training and testing data with a 80/20 split\n"]},{"cell_type":"code","id":"23c7478b-540e-4049-a518-2bf5fca76a52","metadata":{},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)"]},{"cell_type":"markdown","id":"93a77131-1125-4aa4-82a4-37510c44f692","metadata":{},"outputs":[],"source":["Using the same parameter grid as before\n"]},{"cell_type":"code","id":"351f05e6-e542-48ef-8d80-5dc3875eebe2","metadata":{},"outputs":[],"source":["param_grid = {'n_estimators': [2*n+1 for n in range(20)],\n     'base_estimator__max_depth' : [2*n+1 for n in range(10) ]}"]},{"cell_type":"markdown","id":"6c257e28-c051-4e87-8c9c-e29917c5ebae","metadata":{},"outputs":[],"source":["Create a `BaggingClassifier` object called `Bag` with the `base_estimator` set to a `DecisionTreeClassifier` object where `random_state` = 0 and `bootstrap` = True\n"]},{"cell_type":"code","id":"97e0c134-90de-41f5-8ffd-2d1c0ec44354","metadata":{},"outputs":[],"source":["# add your code below\n"]},{"cell_type":"markdown","id":"9b8f8c24-b19c-4df1-a60a-34037439c6e7","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","\n","```python\n","Bag = BaggingClassifier(base_estimator=DecisionTreeClassifier(),random_state=0,bootstrap=True)\n","\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"8d34c1bb-aa91-444d-9ab7-65d7f2a9b6a0","metadata":{},"outputs":[],"source":["Create `GridSearchCV` object called `search` with the `estimator` set to `Bag`, `param_grid` set to `param_grid`, `scoring` set to `accuracy`, and `cv` set to 3.\n"]},{"cell_type":"code","id":"9657f46c-f5e8-4ff5-9e3c-7de6df4e5b71","metadata":{},"outputs":[],"source":["# add your code below"]},{"cell_type":"markdown","id":"765293a5-5851-4432-a7fc-544d1c568d06","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","\n","```python\n","search = GridSearchCV(estimator=Bag, param_grid=param_grid,scoring='accuracy', cv=3)\n","\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"98360001-9749-4dd6-849d-af11f0307692","metadata":{},"outputs":[],"source":["Fit the `GridSearchCV` object to our `X_train` and `y_train` data\n"]},{"cell_type":"code","id":"93845218-8dbf-44da-8966-704d0017ed79","metadata":{},"outputs":[],"source":["# add your code below\n"]},{"cell_type":"markdown","id":"256dd31e-8ef2-4f8b-80fb-497f5cb2eb9c","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","\n","```python\n","search.fit(X_train, y_train)\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"0598d02b-0c48-4913-8b41-24c549e5dcae","metadata":{},"outputs":[],"source":["After using `fit` we can see the best score and parameters\n"]},{"cell_type":"code","id":"ca737e4f-93a9-4dca-8469-e19be3363ce6","metadata":{},"outputs":[],"source":["search.best_score_"]},{"cell_type":"code","id":"b81f86a5-1500-40c0-b11a-9c271e5979d5","metadata":{},"outputs":[],"source":["search.best_params_"]},{"cell_type":"code","id":"aa386daf-a4cf-478e-8150-21dca3efcc4f","metadata":{},"outputs":[],"source":["print(get_accuracy(X_train, X_test, y_train, y_test, search.best_estimator_))"]},{"cell_type":"markdown","id":"19f45bf3-021a-4aad-b8c8-01852b95a997","metadata":{},"outputs":[],"source":["Below we can see a graph of testing and training accuracy holding the max_depth of the base_estimator at 10 and varying the number of estimators. We can see that it is extremely close to the accuracy of the best_estimator we found using GridSearchCV\n"]},{"cell_type":"code","id":"4eebdc8c-2353-4c76-aa10-7f8d2db92d0b","metadata":{},"outputs":[],"source":["get_accuracy_bag(X, y, \"Drug Data\")"]},{"cell_type":"markdown","id":"1c846561-16aa-4ae2-82c5-e3f210756384","metadata":{},"outputs":[],"source":["<h2>Want to learn more?</h2>\n","\n","IBM SPSS Modeler is a comprehensive analytics platform that has many machine learning algorithms. It has been designed to bring predictive intelligence to decisions made by individuals, by groups, by systems – by your enterprise as a whole. A free trial is available through this course, available here: <a href=\"https://www.ibm.com/analytics/spss-statistics-software?utm_source=Exinfluencer&utm_content=000026UJ&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML0101ENSkillsNetwork20718538-2021-01-01&utm_medium=Exinfluencer&utm_term=10006555\">SPSS Modeler</a>\n","\n","Also, you can use Watson Studio to run these notebooks faster with bigger datasets. Watson Studio is IBM's leading cloud solution for data scientists, built by data scientists. With Jupyter notebooks, RStudio, Apache Spark and popular libraries pre-packaged in the cloud, Watson Studio enables data scientists to collaborate on their projects without having to install anything. Join the fast-growing community of Watson Studio users today with a free account at <a href=\"https://www.ibm.com/cloud/watson-studio?utm_source=Exinfluencer&utm_content=000026UJ&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML0101ENSkillsNetwork20718538-2021-01-01&utm_medium=Exinfluencer&utm_term=10006555\">Watson Studio</a>\n"]},{"cell_type":"markdown","id":"b471840a-2b93-4d23-9ff2-1190ad30f9e6","metadata":{},"outputs":[],"source":["### Thank you for completing this lab!\n","\n","## Author\n","\n","<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML0101ENSkillsNetwork20718538-2021-01-01\" target=\"_blank\">Joseph Santarcangelo</a>\n","\n","## Other Contributors\n","\n","<a href=\"https://www.linkedin.com/in/richard-ye/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML0101ENSkillsNetwork20718538-2021-01-01\" target=\"_blank\">Richard Ye</a>\n","\n","## <h3 align=\"center\"> © IBM Corporation 2020. All rights reserved. <h3/>\n","\n","<!--## Change Log\n","\n","| Date (YYYY-MM-DD) | Version | Changed By | Change Description |\n","| ----------------- | ------- | ---------- | ------------------ |\n","| 2022-01-19        | 0.1     | Joseph Santarcangelo | Created Lab Template |\n","| 2022-05-03        | 0.2     | Richard Ye           | QA pass              |--!>\n","\n","\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"prev_pub_hash":"5b56601d636eb85698eaeb203172a15d19563618157128e3afd6f6d3fe5fa3d7"},"nbformat":4,"nbformat_minor":4}