{"cells":[{"cell_type":"markdown","id":"b4166366-894e-46ec-805c-56759bee3b46","metadata":{},"outputs":[],"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","id":"c793033f-23aa-452a-97b1-0c31a692026c","metadata":{},"outputs":[],"source":["# **K Nearest Neighbor**\n"]},{"cell_type":"markdown","id":"c88c10ba-ba76-470f-97d6-dab8f44fba56","metadata":{},"outputs":[],"source":["Estimated time needed: **30** minutes\n"]},{"cell_type":"markdown","id":"b58d4647-381b-4fe7-914b-3285df28747f","metadata":{},"outputs":[],"source":["In this lab, you will learn about and practice the K Nearest Neighbor (KNN) model. KNN is a straightforward but very effective model that can be used for both classification and regression tasks. If the feature space is not very large, KNN can be a high-interpretable model because you can explain and understand how a prediction is made by looking at its nearest neighbors.\n"]},{"cell_type":"markdown","id":"1f95ddc7-b3ca-4609-a95e-1198c315cb49","metadata":{},"outputs":[],"source":["We will be using a tumor sample dataset containing lab test results about tumor samples. The objective is to classify whether a tumor is malicious (cancer) or benign. As such, it is a typical binary classification task.\n"]},{"cell_type":"markdown","id":"0dada1cf-35d2-4198-8bc0-ad83092b7a66","metadata":{},"outputs":[],"source":["## Objectives\n"]},{"cell_type":"markdown","id":"1d5d7347-20b0-42a4-a219-504acce70f5c","metadata":{},"outputs":[],"source":["After completing this lab, you will be able to:\n"]},{"cell_type":"markdown","id":"e3184d60-cb78-43a2-b056-b61d93d99344","metadata":{},"outputs":[],"source":["* Train KNN models with different neighbor hyper-parameters\n","* Evaluate KNN models on classification tasks\n","* Tune the number of neighbors and find the optimized one for a specific task\n"]},{"cell_type":"markdown","id":"d7e0959e-babc-4e2d-a184-ddbc13b8e8e6","metadata":{},"outputs":[],"source":["----\n"]},{"cell_type":"markdown","id":"6447497a-b8e7-4fd1-a208-230f917ffe06","metadata":{},"outputs":[],"source":["First, let's install `seaborn` for visualization tasks and import required libraries for this lab.\n"]},{"cell_type":"code","id":"a3299957-1932-4244-a565-ce2e5356ae00","metadata":{},"outputs":[],"source":["# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n# !mamba install -qy pandas==1.3.3 numpy==1.21.2 ipywidgets==7.4.2 scipy==7.4.2 tqdm==4.62.3 matplotlib==3.5.0 seaborn==0.9.0\n# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\"."]},{"cell_type":"code","id":"4798a187-be0c-45c9-acec-b30412f82937","metadata":{},"outputs":[],"source":["import pandas as pd\nimport numpy as np\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n# Evaluation metrics related methods\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_recall_fscore_support, precision_score, recall_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline"]},{"cell_type":"code","id":"391e5a1a-c8a5-41c1-8039-484fa0f35fca","metadata":{},"outputs":[],"source":["# Define a random seed to reproduce any random process\nrs = 123"]},{"cell_type":"code","id":"32b12a1e-2fb7-47fa-8fd8-dd11026dca45","metadata":{},"outputs":[],"source":["# Ignore any deprecation warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) "]},{"cell_type":"markdown","id":"f0746c67-3d28-4571-95f5-4c687ce8428d","metadata":{},"outputs":[],"source":["## Load and explore the tumor sample dataset\n"]},{"cell_type":"markdown","id":"1f2ca14b-3ce2-4118-8e7c-f584d3576a39","metadata":{},"outputs":[],"source":["We first load the dataset `tumor.csv` as a Pandas dataframe:\n"]},{"cell_type":"code","id":"3d81d97a-70f4-4277-aa8b-714382baf2b8","metadata":{},"outputs":[],"source":["# Read datast in csv format\ndataset_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML241EN-SkillsNetwork/labs/datasets/tumor.csv\"\ntumor_df = pd.read_csv(dataset_url)"]},{"cell_type":"markdown","id":"cd3375f4-a4a6-4592-a3c8-8e3a9e713e20","metadata":{},"outputs":[],"source":["Then, let's quickly take a look at the head of the dataframe.\n"]},{"cell_type":"code","id":"cba40e53-7c71-43bc-85f2-07de2c706e98","metadata":{},"outputs":[],"source":["tumor_df.head()"]},{"cell_type":"markdown","id":"f4e9c328-dddf-4c27-844a-a31db456c43b","metadata":{},"outputs":[],"source":["And, display its columns.\n"]},{"cell_type":"code","id":"6c0f8ee9-a387-4e8a-8acd-2cd627e81802","metadata":{},"outputs":[],"source":["tumor_df.columns"]},{"cell_type":"markdown","id":"d8117bb4-c308-4edd-8d55-a436e39afce8","metadata":{},"outputs":[],"source":["Each observation in this dataset contains lab test results about a tumor sample, such as clump or shapes. Based on these lab test results or features, we want to build a classification model to predict if this tumor sample is malicious (cancer) or benign. The target variable `y` is specified in the `Class` column.\n"]},{"cell_type":"markdown","id":"79f38471-c64a-489c-b96c-6f9b6b90f3c4","metadata":{},"outputs":[],"source":["Then, let's split the dataset into input `X` and output `y`:\n"]},{"cell_type":"code","id":"3ed9c9b3-2fd6-4107-b96d-7e44ed795e70","metadata":{},"outputs":[],"source":["X = tumor_df.iloc[:, :-1]\ny = tumor_df.iloc[:, -1:]"]},{"cell_type":"markdown","id":"9840e187-2780-4155-89d5-5812c3eb5b23","metadata":{},"outputs":[],"source":["And, we first check the statistics summary of features in `X`:\n"]},{"cell_type":"code","id":"7c21c6a0-b368-4e82-8f69-e6f7977d9684","metadata":{},"outputs":[],"source":["X.describe()"]},{"cell_type":"markdown","id":"76795d58-1ee4-4535-a90c-f7174e2bb09b","metadata":{},"outputs":[],"source":["As we can see from the above cell output, all features are numeric and ranged between 1 to 10. This is very convenient as we do not need to scale the feature values as they are already in the same range.\n"]},{"cell_type":"markdown","id":"0e755cb9-ecc1-40de-a391-d9edb09804ec","metadata":{},"outputs":[],"source":["Next, let's check the class distribution of output `y`:\n"]},{"cell_type":"code","id":"cd88b651-97cd-4512-96a5-1224269520b6","metadata":{},"outputs":[],"source":["y.value_counts(normalize=True)"]},{"cell_type":"code","id":"f117d273-8c2f-4178-82e9-ac2b3378eb33","metadata":{},"outputs":[],"source":["y.value_counts().plot.bar(color=['green', 'red'])"]},{"cell_type":"markdown","id":"d1d9a722-0b66-42bd-94fb-c5b880722e77","metadata":{},"outputs":[],"source":["We have about 65% benign tumors (`Class = 0`) and 35% cancerous tumors (`Class = 1`), which is not a very imbalanced class distribution. \n"]},{"cell_type":"markdown","id":"7fc4e0b5-ab70-45d8-8e0f-3ce313ec28e5","metadata":{},"outputs":[],"source":["## Split training and testing datasets\n"]},{"cell_type":"code","id":"8f14bc61-ace9-4c7c-80e2-0e762ee49b7c","metadata":{},"outputs":[],"source":["# Split 80% as training dataset\n# and 20% as testing dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state = rs)"]},{"cell_type":"markdown","id":"d5ef73b6-56e0-492d-b41c-1fbb79ed9cb4","metadata":{},"outputs":[],"source":["## Train and evaluate a KNN classifier with the number of neighbors set to 2\n"]},{"cell_type":"markdown","id":"ef1d5af5-76b8-4363-987b-7fec72169601","metadata":{},"outputs":[],"source":["Training a KNN classifier is very similar to training other classifiers in `sklearn`, we first need to define a `KNeighborsClassifier` object. Here we use `n_neighbors=2` argument to specify how many neighbors will be used for prediction, and we keep other arguments to be their default values.\n"]},{"cell_type":"code","id":"2ee054b9-9f29-4946-8229-5801dd4763c2","metadata":{},"outputs":[],"source":["# Define a KNN classifier with `n_neighbors=2`\nknn_model = KNeighborsClassifier(n_neighbors=2)"]},{"cell_type":"markdown","id":"2d74e437-5a28-4d0e-8a6d-d42c9114a28b","metadata":{},"outputs":[],"source":["Then we can train the model with `X_train` and `y_train`, and we use ravel() method to convert the data frame `y_train` to a vector.\n"]},{"cell_type":"code","id":"e66098bb-8e28-4672-a24d-a6d073feddef","metadata":{},"outputs":[],"source":["knn_model.fit(X_train, y_train.values.ravel())"]},{"cell_type":"markdown","id":"16eb02e4-427e-442f-ac96-bceae1b0eefe","metadata":{},"outputs":[],"source":["And, we can make predictions on the `X_test` dataframe.\n"]},{"cell_type":"code","id":"f522200a-71c2-4981-a1d8-e020810f5376","metadata":{},"outputs":[],"source":["preds = knn_model.predict(X_test)"]},{"cell_type":"markdown","id":"8bbb02c8-d517-42fa-8da1-05dea399ec1a","metadata":{},"outputs":[],"source":["To evaluate the KNN classifier, we provide a pre-defined method to return the commonly used evaluation metrics such as accuracy, recall, precision, f1score, and so on, based on the true classes in the 'y_test' and model predictions.\n"]},{"cell_type":"code","id":"8dd31a9a-d900-437f-ac48-2e02bd2a5810","metadata":{},"outputs":[],"source":["def evaluate_metrics(yt, yp):\n    results_pos = {}\n    results_pos['accuracy'] = accuracy_score(yt, yp)\n    precision, recall, f_beta, _ = precision_recall_fscore_support(yt, yp, average='binary')\n    results_pos['recall'] = recall\n    results_pos['precision'] = precision\n    results_pos['f1score'] = f_beta\n    return results_pos"]},{"cell_type":"code","id":"350adf2b-49d1-4ecd-a461-c83f0e828c66","metadata":{},"outputs":[],"source":["evaluate_metrics(y_test, preds)"]},{"cell_type":"markdown","id":"a07a9d25-54d1-4178-9d96-294d2c86f5a2","metadata":{},"outputs":[],"source":["We can see that there is a great classification performance on the tumor sample dataset. This means the KNN model can effectively recognize cancerous tumors.\n","Next, it's your turn to try a different number of neighbors to see if we could get even better performance.\n"]},{"cell_type":"markdown","id":"3b6b9214-7fbb-419d-9c68-ad92c2e64d34","metadata":{},"outputs":[],"source":["## Coding exercise: Train and evaluate a KNN classifier with number of neighbors set to 5\n"]},{"cell_type":"markdown","id":"5c8629b4-db1b-44de-9d22-8e1e8948a149","metadata":{},"outputs":[],"source":["First, define a KNN classifier with KNeighborsClassifier class:\n"]},{"cell_type":"code","id":"f9a0851d-aca3-4c6b-928e-42ffd3b85cc0","metadata":{},"outputs":[],"source":["# Type your code here\n"]},{"cell_type":"markdown","id":"ba4db56c-a3f0-4c96-947f-cf27a1bf4f09","metadata":{},"outputs":[],"source":["Then train the model with `X_train` and `y_train`:\n"]},{"cell_type":"code","id":"0982b5fe-6fca-4a1f-8c19-7e4b72855ee2","metadata":{},"outputs":[],"source":["# Type your code here\n"]},{"cell_type":"markdown","id":"18b0f174-59f4-43c8-9b98-953e96e613fa","metadata":{},"outputs":[],"source":["And, make predictions on `X_test` dataframe:\n"]},{"cell_type":"code","id":"646ab42d-b917-4c0d-9033-6af27fd9e966","metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","id":"6f880ab2-2e12-43ab-aef0-8b9dd1067df8","metadata":{},"outputs":[],"source":["At last, you can evaluate your KNN model with provided `evaluate_metrics()` method.\n"]},{"cell_type":"markdown","id":"9b15a175-4e68-4ccc-8c25-ab52e78de674","metadata":{},"outputs":[],"source":["<details><summary>Click here for a sample solution</summary>\n","\n","```python\n","model = KNeighborsClassifier(n_neighbors=5)\n","model.fit(X_train, y_train.values.ravel())\n","preds = model.predict(X_test)\n","evaluate_metrics(y_test, preds)\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"b2db2e12-d473-4a46-8b88-760c86b32913","metadata":{},"outputs":[],"source":["## Tune the number of neighbors to find the optmized one\n"]},{"cell_type":"markdown","id":"1601d97c-87d8-4532-abe4-0fa6b8d6fd8c","metadata":{},"outputs":[],"source":["OK, you may wonder which `n_neighbors` argument may give you the best classification performance. We can try different `n_neighbors` (the K value) and check which `K` gives the best classification performance.\n"]},{"cell_type":"markdown","id":"dae2947e-a7d1-4385-940d-69389d694598","metadata":{},"outputs":[],"source":["Here we could try K from 1 to 50, and store the aggregated `f1score` for each k into a list.\n"]},{"cell_type":"code","id":"a48cf517-f914-4da6-81d6-40dd2b519dd2","metadata":{},"outputs":[],"source":["# Try K from 1 to 50\nmax_k = 50\n# Create an empty list to store f1score for each k\nf1_scores = []"]},{"cell_type":"markdown","id":"09db5fd1-b9ff-400b-a97a-98624fc8b4f9","metadata":{},"outputs":[],"source":["Then we will train 50 KNN classifiers with K ranged from 1 to 50.\n"]},{"cell_type":"code","id":"d7912001-c57e-4d28-bddd-5d97ecb0d6fd","metadata":{},"outputs":[],"source":["for k in range(1, max_k + 1):\n    # Create a KNN classifier\n    knn = KNeighborsClassifier(n_neighbors=k)\n    # Train the classifier\n    knn = knn.fit(X_train, y_train.values.ravel())\n    preds = knn.predict(X_test)\n    # Evaluate the classifier with f1score\n    f1 = f1_score(preds, y_test)\n    f1_scores.append((k, round(f1_score(y_test, preds), 4)))\n# Convert the f1score list to a dataframe\nf1_results = pd.DataFrame(f1_scores, columns=['K', 'F1 Score'])\nf1_results.set_index('K')"]},{"cell_type":"markdown","id":"4e102541-e15f-4a23-8dfa-dee7d03c1dff","metadata":{},"outputs":[],"source":["This is a long list and different to analysis, so let's visualize the list using a linechart.\n"]},{"cell_type":"code","id":"6f7f9b0c-ec31-4a34-9d32-6f65a0f5b48e","metadata":{},"outputs":[],"source":["# Plot F1 results\nax = f1_results.plot(figsize=(12, 12))\nax.set(xlabel='Num of Neighbors', ylabel='F1 Score')\nax.set_xticks(range(1, max_k, 2));\nplt.ylim((0.85, 1))\nplt.title('KNN F1 Score')"]},{"cell_type":"markdown","id":"8b52931b-8ca6-4d20-b0fb-775e0e616d41","metadata":{},"outputs":[],"source":["As we can see from the F1 score linechart, the best `K` value is 5 with about `0.9691` f1score.\n"]},{"cell_type":"markdown","id":"20569d1c-9998-461c-8119-0721539d1f42","metadata":{},"outputs":[],"source":["## Next steps\n"]},{"cell_type":"markdown","id":"bd4d382e-9034-4022-9588-d846c8133474","metadata":{},"outputs":[],"source":["Great! Now you have learned about and applied the KNN model to solve a real-world tumor type classification problem. You also tuned the KNN to find the best K value. Later, you will continue learning other popular classification models with different structures, assumptions, cost functions, and application scenarios.\n"]},{"cell_type":"markdown","id":"d5fc95bf-5d2e-4c39-87ed-5010412d48fc","metadata":{},"outputs":[],"source":["## Authors\n"]},{"cell_type":"markdown","id":"e864f446-ca40-4cb1-81e4-687968388406","metadata":{},"outputs":[],"source":["[Yan Luo](https://www.linkedin.com/in/yan-luo-96288783/)\n"]},{"cell_type":"markdown","id":"267ebc3c-9827-4dd9-b31b-8623e1f48328","metadata":{},"outputs":[],"source":["### Other Contributors\n"]},{"cell_type":"markdown","id":"ecd0c404-d93f-4df2-be42-68d0517e4c97","metadata":{},"outputs":[],"source":["<!--## Change Log--!>\n"]},{"cell_type":"markdown","id":"3dd7728f-a9f3-4bca-84b6-837cb14a192f","metadata":{},"outputs":[],"source":["<!--|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n","|-|-|-|-|\n","|2021-11-9|1.0|Yan|Created the initial version|\n","|2022-3-29|1.1|Steve Hord|QA Pass|\n","--!>\n"]},{"cell_type":"markdown","id":"b1807610-0ddd-469c-b725-55768a8b164c","metadata":{},"outputs":[],"source":["Copyright Â© 2021 IBM Corporation. All rights reserved.\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"prev_pub_hash":"3858a67ccb75ce89f71fd5c6b78dfdf49000343a35e71dbe6a6f5081bae93439"},"nbformat":4,"nbformat_minor":4}