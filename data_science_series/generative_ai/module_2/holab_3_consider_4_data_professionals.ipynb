{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "prev_pub_hash": "6ca4ecdb689e464bb101d9105d782fc7ce302962d26c8c5b6f403fcd2bd8be3b"
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "<p style=\"text-align:center\">\n    <a href=\"https://skills.network\" target=\"_blank\">\n    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n    </a>\n</p>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Hands-on Lab: Considerations for Data Professionals using GenAI\n\n**Estimated time needed:** 45 minutes \n\n## Overview  \n\nIn this lab, you will assess and reinforce your understanding of key principles related to the ethical deployment of generative AI, specifically focusing on transparency, fairness, responsibility, accountability, and reliability.  \n\nYou will be presented with scenarios, and you are expected to provide a solution for the question based on the scenario. To help you with the solutions, a hint is provided for each exercise. \n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "---\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Learning Objectives \n\nAfter completing this lab, you will be able to: \n\n - Maintain transparency and fairness in your AI system \n\n - Ensure accountability in the deployment of AI chatbot \n\n - Enhance the reliability of your AI model to ensure accurate product descriptions \n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "---\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Exercise 1:  \n\nYou are developing a generative AI system that creates personalized content recommendations for users. The system seems to consistently recommend content that aligns with certain cultural and demographic biases.  \n\nUsers from diverse backgrounds are expressing concern about the lack of transparency and fairness in the recommendations.  \n\nHow do you maintain transparency and fairness in your AI system? \n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<details><summary>Click here for solution</summary>\nTo maintain transparency and fairness in an AI system, the following steps can be implemented:\n\n1. __Conduct Bias Assessment__:\n    * Perform a thorough bias analysis to identify any inherent biases in the training data or algorithm.\n    * Use bias detection tools and metrics to regularly evaluate the system's outputs for potential discrimination.\n2. __Measure and Quantify Bias__:\n    * Implement metrics to measure bias across different dimensions, such as demographics, geography, and socio-economic status.\n    * Ensure continuous tracking of fairness in AI recommendations and decisions.\n3. __Enhance Data Diversity__:\n    * Improve the diversity of the training data by incorporating a broad range of cultural, demographic, and behavioral data.\n    * Ensure that the training data reflects a balanced representation of different user groups.\n4. __Use Explainability Features__:\n    * Provide explainability tools (e.g., LIME or SHAP) to help users understand the rationale behind AI-driven decisions.\n    * Offer transparency by displaying the factors that contributed to a decision, improving trust and clarity.\n5. __Feedback Loop__:\n    * Create a user feedback loop that allows users to report concerns related to bias or incorrect decisions.\n    * Collect, analyze, and implement feedback to iteratively improve the system's fairness and transparency.\n6. __Regular Audits__:\n    * Conduct periodic audits to ensure that the AI system remains fair, especially after updates.\n    * Adjust models as new biases or fairness issues are discovered.\n7. __Ongoing Monitoring__:\n    * Continuously monitor AI outcomes to detect any unintended biases that may emerge over time.\n    * Refine models based on real-world performance and user interactions.\n   \nBy integrating these steps, AI systems can promote fairness, reduce bias, and maintain transparent decision-making that aligns with ethical standards.\n</details>",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<details><summary>Click here for hint</summary>\nConsider steps like conducting a bias assessment, enhancing diversity in training data, implementing explainability features, and establishing a user feedback loop to ensure fairness and transparency in your AI system. \n</details>\n\n<details><summary>Click here for sample solution</summary>\nTo address this issue, you could implement the following steps: \n\n1. Conduct a thorough bias assessment to identify and understand the biases present in the training data and algorithms. \n2. Use specialized tools or metrics to measure and quantify biases in content recommendations. \n3. Enhance the diversity of your training data by including a broader range of cultural, demographic, and user behavior data. \n4. Ensure that the training data reflects the diversity of your user base to reduce biases. \n5. Implement explainability features to provide users with insights into why specific recommendations are made. \n6. Offer transparency by showing the key factors and attributes influencing the recommendations. \n7. Establish a user feedback loop where users can report biased recommendations or provide feedback on content relevance. \n8. Regularly analyze this feedback to iteratively improve the system's fairness. \n</details>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">**Additional Information**\n\n>Some specialized tools that can be used to measure and quantify biases: \n\n>Holistic AI Library: This open-source library offers a range of metrics and mitigation strategies for various AI tasks, including content recommendation. It analyzes data for bias across different dimensions and provides visualizations for clear understanding. \n\n>Fairness 360: IBM&#39;s Fairness 360 toolkit provides various tools like Aequitas and What-If Tool to analyze bias in data sets, models, and decision-making processes. It offers metrics like statistical parity, differential odds ratio, and counterfactual fairness. IBM moved AI Fairness 360 to LF AI in July 2020. \n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Exercise 2  \n\nYour company has deployed a chatbot powered by generative AI to interact with customers. The chatbot occasionally generates responses that are inappropriate or offensive, leading to customer dissatisfaction. As the AI developer, how do you take responsibility for these incidents and ensure accountability in the deployment of the AI chatbot? \n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<details><summary>Click here for solution</summary>\nTo ensure responsibility and accountability for the AI chatbot's inappropriate responses, the following steps can be implemented:\n\n1. __Analyze Inappropriate Responses__\n    * Conduct a detailed analysis of the incidents to identify patterns and root causes (e.g., biased training data, algorithm limitations).\n    * Use tools to determine whether the issue lies in data quality, model behavior, or response generation logic.\n2. __Rectify Training Data and Model__\n    * Implement a process to update training data to address biases or gaps in coverage.\n    * Fine-tune the model to handle sensitive topics more appropriately and minimize offensive outputs.\n3. __Implement Monitoring and Alerts__\n    * Establish real-time monitoring to detect and flag inappropriate or offensive responses.\n    * Implement a human-in-the-loop system that allows for human intervention when the chatbot generates potentially harmful content.\n4. __Swiftly Address Issues__\n    * Respond quickly to rectify issues, update the model, and communicate actions taken to prevent future occurrences.\n    * Ensure corrective measures are in place to immediately improve the chatbot's behavior after each incident.\n5. __Open Communication__\n    * Acknowledge the issue to affected customers and provide transparent explanations of the corrective actions taken.\n    * Reassure customers of the commitment to improve the AI's performance and ensure a positive user experience.\n6. __Continuous Monitoring and Updates__\n    * Set up ongoing systems for continuous learning and regular audits of the chatbot's performance to prevent future errors.\n    * Regularly update training data and refine model behavior to align with evolving customer expectations.\n7. __Stakeholder Communication__\n    * Clearly communicate with internal stakeholders and customers on steps taken, reinforcing the company's commitment to ethical AI deployment.\n      \nBy taking proactive steps, acknowledging errors, and ensuring continuous improvement, you can maintain accountability and enhance the chatbot's trustworthiness and performance.\n</details>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<details><summary>Click here for hint</summary>\nTo address responsibility and accountability, analyze errors, respond swiftly, continuously monitor for inappropriate responses, and communicate openly with stakeholders about corrective actions taken to improve the chatbot&#39;s behavior. \n</details>\n\n<details><summary>Click here for sample solution</summary>\nAddressing responsibility and accountability in this scenario involves the following steps: \n\n1. Conduct a detailed analysis of the inappropriate responses to identify patterns and root causes. \n2. Determine whether the issues stem from biased training data, algorithmic limitations, or other factors. \n3. Implement a mechanism to quickly identify and rectify inappropriate responses by updating the chatbot&#39;s training data or fine-tuning the model. \n4. Communicate openly with affected customers, acknowledge the issue, and assure them of prompt corrective actions. \n5. Set up continuous monitoring systems to detect and flag inappropriate responses in real-time. \n6. Implement alerts or human-in-the-loop mechanisms to intervene when the system generates potentially harmful content. \n7. Clearly communicate the steps taken to address the issue to both internal stakeholders and customers. \n8. Emphasize the commitment to continuous improvement and the responsible use of AI technology. \n</details>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Exercise 3: \n\nYour company has developed a generative AI model that autonomously generates product descriptions for an e-commerce platform. However, users have reported instances where the generated descriptions contain inaccurate information, leading to customer confusion and dissatisfaction. How do you enhance the reliability of your AI model to ensure accurate product descriptions? \n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<details><summary>Click here for solution</summary>\nTo enhance the reliability of the AI model generating product descriptions, the following steps can be implemented:\n\n1. __Quality Assurance Testing__\n    * Implement rigorous quality assurance by evaluating the generated descriptions for accuracy before deployment.\n    * Develop a comprehensive test dataset covering a wide range of products, ensuring the model is tested across various product categories and scenarios.\n2. __Domain-Specific Training Data__\n    * Train the model on diverse, domain-specific data that includes accurate and up-to-date product information.\n    * Incorporate trusted sources (e.g., manufacturer websites, verified product databases) to ensure the model generates reliable descriptions.\n3. __Iterative Model Training__\n    * Adopt an iterative approach to model training, continuously updating the model based on user feedback and new product data.\n    * Regularly retrain the model to reflect changes in the product catalog, keeping it up-to-date with new product features and categories.\n4. __User Feedback Integration__\n    * Encourage user feedback on inaccurate descriptions and integrate this feedback into the model's learning process.\n    * Fine-tune the model based on recurring errors or inaccuracies to continuously improve output quality.\n5. __Continuous Monitoring___\n    * Set up monitoring systems to track the performance of the AI-generated descriptions, flagging inaccuracies for immediate review and correction.\n    * Use human oversight for critical product categories to ensure the descriptions are accurate and align with business standards.  \n      \nBy focusing on quality assurance, domain-specific training, and iterative improvements, along with incorporating user feedback, the AI model can become more reliable and generate accurate, helpful product descriptions that improve user satisfaction and reduce confusion.\n</details>",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<details><summary>Click here for hint</summary>\nTo improve reliability, focus on quality assurance testing, use domain-specific training data, adopt an iterative model training approach, and integrate user feedback to iteratively correct errors and enhance the accuracy of the AI-generated product descriptions. \n</details>\n\n<details><summary>Click here for sample solution</summary>\nTo improve the reliability of the AI model in generating product descriptions, consider the following actions: \n\n1. Implement rigorous quality assurance testing to evaluate the accuracy of the generated product descriptions. \n2. Create a comprehensive testing data set that covers a wide range of products and scenarios to identify and address inaccuracies. \n3. Ensure that the AI model is trained on a diverse and extensive data set specific to the e-commerce domain. \n4. Include product information from reputable sources to enhance the model's understanding of accurate product details. \n5. Implement an iterative training approach to continuously update and improve the model based on user feedback and evolving product data. \n6. Regularly retrain the model to adapt to changes in the product catalog and user preferences. \n7. Encourage users to provide feedback on inaccurate product descriptions. \n8. Use this feedback to fine-tune the model, correct errors, and improve the overall reliability of the AI-generated content.  \n</details>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "---\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Congratulations! You have completed the lab\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Authors\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "[Dr. Pooja](https://www.linkedin.com/in/p-b28802262/)\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Other Contributors\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "[Abhishek Gagneja](https://www.linkedin.com/in/abhishek-gagneja-23051987/)\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<!--## Change Log--!>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<!--|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n|-|-|-|-|\n|2023-12-14|0.1|Abhishek Gagneja|Initial Draft created| --!>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Copyright Â© 2023 IBM Corporation. All rights reserved.\n",
      "metadata": {}
    }
  ]
}